{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sourav/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# LB of xx\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import load_img, img_to_array, list_pictures\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau , LearningRateScheduler\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import load_model \n",
    "from keras.layers import Concatenate,Input,ZeroPadding2D,Dense,Flatten,GlobalAveragePooling2D,AveragePooling2D, SpatialDropout2D,Multiply,Conv2D,UpSampling2D, Conv2DTranspose, MaxPooling2D, concatenate,Reshape,RepeatVector, Dropout, BatchNormalization,Add\n",
    "from keras.layers.core import Activation\n",
    "from keras import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "from skimage.transform import resize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.backend import tf as ktf\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from clr_callback import *\n",
    "from lr_new import LRFinder\n",
    "\n",
    "import cv2\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from scipy.ndimage.filters import gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\", index_col=\"id\", usecols=[0])\n",
    "depths_df = pd.read_csv(\"depths.csv\", index_col=\"id\")\n",
    "train_df = train_df.join(depths_df)\n",
    "test_df = depths_df[~depths_df.index.isin(train_df.index)]\n",
    "train_df[\"images\"] = [np.array(load_img(\"images/{}.png\".format(idx),grayscale = True)) / 255 for idx in train_df.index]\n",
    "train_df[\"masks\"] = [np.array(load_img(\"masks/{}.png\".format(idx),grayscale = True)) / 255 for idx in train_df.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Remove images with 1px\n",
    "train_df = train_df[train_df.masks.map(np.sum) != 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"masks_flag\"] = (train_df.masks.map(np.sum) > 0)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifying empty images\n",
    "x_train_nn = train_df[[\"images\",\"masks_flag\",\"masks\"]].copy()\n",
    "x_train_nn = x_train_nn[x_train_nn.masks_flag.values>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_interpolate(A, new_size):\n",
    "    \"\"\"Vectorized Nearest Neighbor Interpolation\"\"\"\n",
    "\n",
    "    old_size = A.shape\n",
    "    row_ratio, col_ratio = np.array(new_size)/np.array(old_size)\n",
    "\n",
    "    # row wise interpolation \n",
    "    row_idx = (np.ceil(range(1, 1 + int(old_size[0]*row_ratio))/row_ratio) - 1).astype(int)\n",
    "\n",
    "    # column wise interpolation\n",
    "    col_idx = (np.ceil(range(1, 1 + int(old_size[1]*col_ratio))/col_ratio) - 1).astype(int)\n",
    "\n",
    "    final_matrix = A[:, row_idx][col_idx, :]\n",
    "\n",
    "    return final_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change image size from X*Y to X*X\n",
    "img_size_ori = 101\n",
    "img_size_target = 128\n",
    "\n",
    "def upsample(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n",
    "    \n",
    "def downsample(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"coverage\"] = train_df.masks.map(np.sum) / pow(img_size_ori, 2)\n",
    "#x_train_nn [\"coverage\"] = x_train_nn.masks.map(np.sum) / pow(img_size_ori, 2)\n",
    "def get_mask_type(mask):\n",
    "    border = 10\n",
    "    outer = np.zeros((101-2*border, 101-2*border), np.float32)\n",
    "    outer = cv2.copyMakeBorder(outer, border, border, border, border, borderType = cv2.BORDER_CONSTANT, value = 1)\n",
    "\n",
    "    cover = (mask>0.5)*1\n",
    "    cover = sum([cover])\n",
    "    #print(cover)\n",
    "    if cover < 8:\n",
    "        return 0 # empty\n",
    "    if cover == ((mask*outer) > 0.5).sum():\n",
    "        return 1 #border\n",
    "    if np.all(mask==mask[0]):\n",
    "        return 2 #vertical\n",
    "\n",
    "    percentage = cover/(101*101)\n",
    "    if percentage < 0.15:\n",
    "        return 3\n",
    "    elif percentage < 0.25:\n",
    "        return 4\n",
    "    elif percentage < 0.50:\n",
    "        return 5\n",
    "    elif percentage < 0.75:\n",
    "        return 6\n",
    "    else:\n",
    "        return 7\n",
    "\n",
    "def histcoverage(coverage):\n",
    "    histall = np.zeros((1,8))\n",
    "    for c in coverage:\n",
    "        histall[0,c] += 1\n",
    "    return histall\n",
    "        \n",
    "\n",
    "train_df[\"coverage_class\"] = train_df.coverage.map(get_mask_type) # https://www.kaggle.com/shaojiaxin/u-net-resnet-v3-stratifiedkfold\n",
    "#x_train_nn[\"coverage_class\"] = train_df.coverage.map(get_mask_type) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_train, ids_valid, x_train, x_valid,  y_train, y_valid,X_feat_train, X_feat_valid,y_train_hasmask,y_valid_hasmask = train_test_split(\n",
    "    train_df.index.values,\n",
    "    np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target,1), \n",
    "    np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n",
    "    train_df.z.values,\n",
    "    train_df.masks_flag.values,\n",
    "    test_size=0.1,stratify=train_df.coverage_class, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from augmentation import iaa, PadFixed, RandomCropFixedSize\n",
    "from utils import plot_list\n",
    "seq = iaa.Sequential([\n",
    "    iaa.Fliplr(0.5), # horizontally flip\n",
    "    iaa.OneOf([\n",
    "        iaa.Noop(),\n",
    "        iaa.GaussianBlur(sigma=(0.0, 1.0)),\n",
    "        iaa.Noop(),\n",
    "        iaa.Affine(rotate=(-10, 10), translate_percent={\"x\": (-0.25, 0.25)}, mode='symmetric', cval=(0)),\n",
    "        iaa.Noop(),\n",
    "        iaa.PerspectiveTransform(scale=(0.04, 0.08)),\n",
    "        iaa.Noop(),\n",
    "        iaa.PiecewiseAffine(scale=(0.05, 0.1), mode='edge', cval=(0)),\n",
    "    ])\n",
    "])\n",
    "\n",
    "seq_det = seq.to_deterministic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUG_NR = 6\n",
    "aug_imgs_x = []\n",
    "aug_imgs_y = []\n",
    "for i in range(len(x_train)-1):\n",
    "    for _ in range(AUG_NR):\n",
    "        img_x = seq_det.augment_image(x_train[i])\n",
    "        img_y = seq_det.augment_image(y_train[i])\n",
    "        \n",
    "        aug_imgs_x.append(img_x),aug_imgs_y.append(img_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_imgs_x = np.array(aug_imgs_x)\n",
    "aug_imgs_y = np.array(aug_imgs_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.append(x_train,aug_imgs_x,axis=0)\n",
    "y_train = np.append(y_train,aug_imgs_y,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal flip\n",
    "x_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\n",
    "y_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)\n",
    "#x_train_n = np.append(x_train_n,[np.fliplr(x) for x in x_train_n], axis=0)\n",
    "#y_train_n = np.append(y_train_n,y_train_n , axis=0)\n",
    "y_train_hasmask = np.append(y_train_hasmask,y_train_hasmask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_hasmask = np.zeros((len(y_train)))\n",
    "for i in range(len(y_train)-1):\n",
    "    y_train_hasmask[i] =  (y_train[i,:,:,0].sum()>0)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7180, 128, 128, 1), (7180, 128, 128, 1), (7180,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape,y_train_hasmask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage\n",
    "def add_channel(x_block):\n",
    "    new_x_train = np.zeros((len(x_block), img_size_target, img_size_target, 3))\n",
    "    for i in range(x_block.shape[0]):\n",
    "        new_x_train[i,:,:,0] =  x_block[i,: ,:,0]\n",
    "        h,w,n = x_block[i,: ,:,:].shape\n",
    "        for row, const in enumerate(np.linspace(0, 1, h)):\n",
    "            new_x_train[i,row, : ,1] = const\n",
    "        new_x_train[i,:,:,2] = ndimage.sobel(x_block[i,: ,:,0], axis=-1, output=None, mode='nearest')#new_x_train[i,:,:,0] * new_x_train[i,:,:,1]\n",
    "        #new_x_train[i,:,:,3] = ndimage.sobel(x_block[i,: ,:,0], axis=-1, output=None, mode='nearest')\n",
    "    x_block = new_x_train\n",
    "    del new_x_train\n",
    "    return x_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = add_channel(x_train)\n",
    "x_valid = add_channel(x_valid)\n",
    "#x_train_n = add_channel(x_train_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_crossentropy(target, output):\n",
    "    _epsilon = 10e-8\n",
    "    pos_weight = 0.75\n",
    "    neg_weight = 0.25\n",
    "    output = K.sigmoid(output)\n",
    "    output = tf.clip_by_value(output, _epsilon, 1. - _epsilon)\n",
    "    return - tf.reduce_sum(target * pos_weight *  (1. - output) * tf.log(output) +\n",
    "                           (1 - target)  *  neg_weight * output * tf.log(1 - output),\n",
    "                           len(output.get_shape()) - 1)\n",
    "\n",
    "def my_crossentropy_non_empty(target, output):\n",
    "    targetf = tf.cast(target, output.dtype)\n",
    "    targets = tf.reduce_sum(targetf, axis=[1,2,3])\n",
    "    losses = my_crossentropy(target, output)\n",
    "    return tf.where(targets > 0,\n",
    "                    losses,\n",
    "                    0*losses\n",
    "                   )\n",
    "\n",
    "\n",
    "#focal loss\n",
    "def focal_loss_fixed(y_true, y_pred):\n",
    "    gamma=2.0\n",
    "    alpha=0.2\n",
    "    pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "    pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "    return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "\n",
    "#Dice loss calculation\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    y_pred_f = K.cast(K.greater(K.flatten(y_pred), 0.5), 'float32')\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = 2. * K.sum(intersection) / (K.sum(y_true_f) + K.sum(y_pred_f))\n",
    "    return score\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return 1. - score\n",
    "\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return (binary_crossentropy(y_true, y_pred) + 2*dice_loss(y_true, y_pred))\n",
    "\n",
    "def focal_dice_loss(y_true, y_pred):\n",
    "    return (focal_loss_fixed(y_true, y_pred) + 2*dice_loss(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iou_vector(A, B):\n",
    "    # Numpy version\n",
    "    \n",
    "    batch_size = A.shape[0]\n",
    "    metric = 0.0\n",
    "    for batch in range(batch_size):\n",
    "        t, p = A[batch], B[batch]\n",
    "        true = np.sum(t)\n",
    "        pred = np.sum(p)\n",
    "        \n",
    "        # deal with empty mask first\n",
    "        if true == 0:\n",
    "            metric += (pred == 0)\n",
    "            continue\n",
    "        \n",
    "        # non empty mask case.  Union is never empty \n",
    "        # hence it is safe to divide by its number of pixels\n",
    "        intersection = np.sum(t * p)\n",
    "        union = true + pred - intersection\n",
    "        iou = intersection / union\n",
    "        \n",
    "        # iou metrric is a stepwise approximation of the real iou over 0.5\n",
    "        iou = np.floor(max(0, (iou - 0.45)*20)) / 10\n",
    "        \n",
    "        metric += iou\n",
    "        \n",
    "    # teake the average over all images in batch\n",
    "    metric /= batch_size\n",
    "    return metric\n",
    "\n",
    "\n",
    "def my_iou_metric(label, pred):\n",
    "    # Tensorflow version\n",
    "    return tf.py_func(get_iou_vector, [label, pred > 0.5], tf.float64)\n",
    "\n",
    "def my_iou_metric_2(label, pred):\n",
    "    return tf.py_func(get_iou_vector, [label, pred >0], tf.float64)\n",
    "\n",
    "\n",
    "\n",
    "def my_iou_metric_non_zero(label, pred):\n",
    "    # Tensorflow version\n",
    "    return tf.py_func(get_iou_vector, [label, pred > 0.5], tf.float64)\n",
    "\n",
    "\n",
    "def get_iou_vector_non_zero(A, B):\n",
    "    # Numpy version\n",
    "    k = 0\n",
    "    batch_size = A.shape[0]\n",
    "    metric = 0.0\n",
    "    for batch in range(batch_size):\n",
    "        t, p = A[batch], B[batch]\n",
    "        true = np.sum(t)\n",
    "        pred = np.sum(p)\n",
    "        \n",
    "        # deal with empty mask first\n",
    "        if true != 0:\n",
    "            \n",
    "            # non empty mask case.  Union is never empty \n",
    "            # hence it is safe to divide by its number of pixels\n",
    "            intersection = np.sum(t * p)\n",
    "            union = true + pred - intersection\n",
    "            #print(union)\n",
    "            iou = intersection / union\n",
    "\n",
    "            # iou metrric is a stepwise approximation of the real iou over 0.5\n",
    "            iou = np.floor(max(0, (iou - 0.45)*20)) / 10\n",
    "\n",
    "            metric += iou\n",
    "            k += 1\n",
    "            \n",
    "            print(\"iou\",iou,\"k\",k)\n",
    "            \n",
    "    # teake the average over all images in batch\n",
    "    metric /= k\n",
    "    return metric\n",
    "\n",
    "\n",
    "\n",
    "def mean_iou(y_true, y_pred):\n",
    "    #print(y_true,y_pred)\n",
    "    prec = []\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        y_pred_ = tf.to_int32(y_pred > t)\n",
    "        score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([up_opt]):\n",
    "            score = tf.identity(score)\n",
    "        prec.append(score)\n",
    "    return K.mean(K.stack(prec), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code download from: https://github.com/bermanmaxim/LovaszSoftmax\n",
    "def lovasz_grad(gt_sorted):\n",
    "    \"\"\"\n",
    "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
    "    See Alg. 1 in paper\n",
    "    \"\"\"\n",
    "    gts = tf.reduce_sum(gt_sorted)\n",
    "    intersection = gts - tf.cumsum(gt_sorted)\n",
    "    union = gts + tf.cumsum(1. - gt_sorted)\n",
    "    jaccard = 1. - intersection / union\n",
    "    jaccard = tf.concat((jaccard[0:1], jaccard[1:] - jaccard[:-1]), 0)\n",
    "    return jaccard\n",
    "\n",
    "\n",
    "# --------------------------- BINARY LOSSES ---------------------------\n",
    "\n",
    "def lovasz_hinge(logits, labels, per_image=True, ignore=None):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
    "      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n",
    "      per_image: compute the loss per image instead of per batch\n",
    "      ignore: void class id\n",
    "    \"\"\"\n",
    "    if per_image:\n",
    "        def treat_image(log_lab):\n",
    "            log, lab = log_lab\n",
    "            log, lab = tf.expand_dims(log, 0), tf.expand_dims(lab, 0)\n",
    "            log, lab = flatten_binary_scores(log, lab, ignore)\n",
    "            return lovasz_hinge_flat(log, lab)\n",
    "        losses = tf.map_fn(treat_image, (logits, labels), dtype=tf.float32)\n",
    "        loss = tf.reduce_mean(losses)\n",
    "    else:\n",
    "        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def lovasz_hinge_flat(logits, labels):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
    "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
    "      ignore: label to ignore\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_loss():\n",
    "        labelsf = tf.cast(labels, logits.dtype)\n",
    "        signs = 2. * labelsf - 1.\n",
    "        errors = 1. - logits * tf.stop_gradient(signs)\n",
    "        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], name=\"descending_sort\")\n",
    "        gt_sorted = tf.gather(labelsf, perm)\n",
    "        grad = lovasz_grad(gt_sorted)\n",
    "        loss = tf.tensordot(tf.nn.relu(errors_sorted), tf.stop_gradient(grad), 1, name=\"loss_non_void\")\n",
    "        return loss\n",
    "\n",
    "    # deal with the void prediction case (only void pixels)\n",
    "    loss = tf.cond(tf.equal(tf.shape(logits)[0], 0),\n",
    "                   lambda: tf.reduce_sum(logits) * 0.,\n",
    "                   compute_loss,\n",
    "                   strict=True,\n",
    "                   name=\"loss\"\n",
    "                   )\n",
    "    return loss\n",
    "\n",
    "\n",
    "def flatten_binary_scores(scores, labels, ignore=None):\n",
    "    \"\"\"\n",
    "    Flattens predictions in the batch (binary case)\n",
    "    Remove labels equal to 'ignore'\n",
    "    \"\"\"\n",
    "    scores = tf.reshape(scores, (-1,))\n",
    "    labels = tf.reshape(labels, (-1,))\n",
    "    if ignore is None:\n",
    "        return scores, labels\n",
    "    valid = tf.not_equal(labels, ignore)\n",
    "    vscores = tf.boolean_mask(scores, valid, name='valid_scores')\n",
    "    vlabels = tf.boolean_mask(labels, valid, name='valid_labels')\n",
    "    return vscores, vlabels\n",
    "\n",
    "def lovasz_loss(y_true, y_pred):\n",
    "    y_true, y_pred = K.cast(K.squeeze(y_true, -1), 'int32'), K.cast(K.squeeze(y_pred, -1), 'float32')\n",
    "    #logits = K.log(y_pred / (1. - y_pred))\n",
    "    logits = y_pred #Jiaxin\n",
    "    loss = lovasz_hinge(logits, y_true, per_image = True, ignore = None)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squeeze and excite layer\n",
    "def sc_se_block(in_block, ch, ratio=16):\n",
    "    \n",
    "    # Squeeze block\n",
    "    x_sq = GlobalAveragePooling2D()(in_block)\n",
    "    x_sq = Dense(ch//ratio, activation='relu')(x_sq)\n",
    "    x_sq = Dense(ch, activation='sigmoid')(x_sq)\n",
    "    \n",
    "    x_sq = Multiply()([in_block, x_sq])\n",
    "    \n",
    "    # Channel Block\n",
    "    \n",
    "    x_ch = Conv2D(1, (1, 1), activation=\"sigmoid\")(in_block)\n",
    "    x_ch = Multiply()([in_block, x_ch])\n",
    "        \n",
    "    return Add()([x_sq, x_ch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_layer(F_int,g,x ,namex):\n",
    "    g1 = Conv2D(F_int, kernel_size=(3,3),strides=(1,1),padding=\"same\" , name = namex + 'Convgate')(g)\n",
    "    g1 = BatchNormalization()(g1)\n",
    "    \n",
    "    x1 = Conv2D(F_int, kernel_size=(3,3),strides=(1,1),padding=\"same\",name = namex + 'Convresidual')(x)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    \n",
    "    psi = Add()([g1,x1])\n",
    "    psi = Activation('relu')(psi)\n",
    "    psi = Conv2D(1,kernel_size=(3,3),strides=(1,1),padding=\"same\" , name = namex + 'Convadd')(psi)\n",
    "    psi = BatchNormalization()(psi)\n",
    "    psi = Activation('sigmoid')(psi)\n",
    "    \n",
    "    return Multiply()([x1,psi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "side_block_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side Layer\n",
    "def side_branch(x_block, factor):\n",
    "    \n",
    "    x_side = Conv2D(1, (1, 1), activation=None, padding='same')(x_block)\n",
    "    kernel_size = (2*factor, 2*factor)\n",
    "    x_side_up = Conv2DTranspose(1,kernel_size, strides =(2*factor, 2*factor), padding = 'same' )(x_side)\n",
    "    #print('side_out_shape',x_side_up.shape)\n",
    "    \n",
    "    return x_side_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_stage_names(stage):\n",
    "    #print(stage)\n",
    "    conv_name = 'decoder_stage{}_conv'.format(stage)\n",
    "    bn_name = 'decoder_stage{}_bn'.format(stage)\n",
    "    relu_name = 'decoder_stage{}_relu'.format(stage)\n",
    "    up_name = 'decoder_stage{}_upsample'.format(stage)\n",
    "    return conv_name, bn_name, relu_name, up_name\n",
    "\n",
    "\n",
    "def ConvRelu(filters, kernel_size, use_batchnorm=False, conv_name='conv', bn_name='bn', relu_name='relu'):\n",
    "    def layer(x):\n",
    "        x = Conv2D(filters, kernel_size, padding=\"same\", name=conv_name, use_bias=not(use_batchnorm))(x)\n",
    "        if use_batchnorm:\n",
    "            x = BatchNormalization(name=bn_name)(x)\n",
    "        x = Activation('relu', name=relu_name)(x)\n",
    "        return x\n",
    "    return layer\n",
    "\n",
    "\n",
    "def Upsample2D_block(filters, stage, kernel_size=(3,3), upsample_rate=(2,2),\n",
    "                     use_batchnorm=False, skip=None):\n",
    "\n",
    "    def layer(input_tensor):\n",
    "        #print(stage)\n",
    "        conv_name, bn_name, relu_name, up_name = handle_stage_names(stage)\n",
    "\n",
    "        x = UpSampling2D(size=upsample_rate, name=up_name)(input_tensor)\n",
    "\n",
    "        if skip is not None:\n",
    "            att = attention_layer(filters,x,skip , up_name+'_att')\n",
    "            x = concatenate([x, att])\n",
    "\n",
    "        x = ConvRelu(filters, kernel_size, use_batchnorm=use_batchnorm,\n",
    "                     conv_name=conv_name + '1', bn_name=bn_name + '1', relu_name=relu_name + '1')(x)\n",
    "\n",
    "        x = ConvRelu(filters, kernel_size, use_batchnorm=use_batchnorm,\n",
    "                     conv_name=conv_name + '2', bn_name=bn_name + '2', relu_name=relu_name + '2')(x)\n",
    "\n",
    "        return x\n",
    "    return layer\n",
    "\n",
    "\n",
    "def Transpose2D_block(filters, stage, kernel_size=(3,3), upsample_rate=(2,2),\n",
    "                      transpose_kernel_size=(4,4), use_batchnorm=False, skip=None):\n",
    "\n",
    "    def layer(input_tensor):\n",
    "\n",
    "        conv_name, bn_name, relu_name, up_name = handle_stage_names(stage)\n",
    "\n",
    "        x = Conv2DTranspose(filters, transpose_kernel_size, strides=upsample_rate,\n",
    "                            padding='same', name=up_name, use_bias=not(use_batchnorm))(input_tensor)\n",
    "        if use_batchnorm:\n",
    "            x = BatchNormalization(name=bn_name+'1')(x)\n",
    "        x = Activation('relu', name=relu_name+'1')(x)\n",
    "\n",
    "        if skip is not None:\n",
    "            x = concatenate([x, skip])\n",
    "\n",
    "        x = ConvRelu(filters, kernel_size, use_batchnorm=use_batchnorm,\n",
    "                     conv_name=conv_name + '2', bn_name=bn_name + '2', relu_name=relu_name + '2')(x)\n",
    "\n",
    "        return x\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_number(model, layer_name):\n",
    "    \"\"\"\n",
    "    Help find layer in Keras model by name\n",
    "    Args:\n",
    "        model: Keras `Model`\n",
    "        layer_name: str, name of layer\n",
    "    Returns:\n",
    "        index of layer\n",
    "    Raises:\n",
    "        ValueError: if model does not contains layer with such name\n",
    "    \"\"\"\n",
    "    for i, l in enumerate(model.layers):\n",
    "        if l.name == layer_name:\n",
    "            return i\n",
    "    raise ValueError('No layer with name {} in  model {}.'.format(layer_name, model.name))\n",
    "\n",
    "def to_tuple(x):\n",
    "    if isinstance(x, tuple):\n",
    "        if len(x) == 2:\n",
    "            return x\n",
    "    elif np.isscalar(x):\n",
    "        return (x, x)\n",
    "    \n",
    "def get_model_output(model,layer_name):\n",
    "    for i,l in enumerate(model.layers):\n",
    "        print(\"All labels:\" , l.name)\n",
    "        if l.name == layer_name:\n",
    "            #print(layer_name)\n",
    "            #output_val = l.output\n",
    "            return output_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unet(backbone, classes, skip_connection_layers,\n",
    "               decoder_filters=(256,128,64,32,16),\n",
    "               upsample_rates=(2,2,2,2,2),\n",
    "               n_upsample_blocks=5,\n",
    "               block_type='upsampling',\n",
    "               activation='sigmoid',\n",
    "               use_batchnorm=True):\n",
    "\n",
    "    input_x = backbone.input\n",
    "    x = backbone.output\n",
    "    \n",
    "    hyper_col_name = {}\n",
    "    \n",
    "    if block_type == 'transpose':\n",
    "        up_block = Transpose2D_block\n",
    "    else:\n",
    "        up_block = Upsample2D_block\n",
    "\n",
    "    # convert layer names to indices\n",
    "    skip_connection_idx = ([get_layer_number(backbone, l) if isinstance(l, str) else l\n",
    "                               for l in skip_connection_layers])\n",
    "\n",
    "    for i in range(n_upsample_blocks):\n",
    "        \n",
    "        # check if there is a skip connection\n",
    "        skip_connection = None\n",
    "        if i < len(skip_connection_idx):\n",
    "            skip_connection = backbone.layers[skip_connection_idx[i]].output\n",
    "\n",
    "        upsample_rate = to_tuple(upsample_rates[i])\n",
    "\n",
    "        x = Upsample2D_block(decoder_filters[i], i, upsample_rate=upsample_rate,\n",
    "                     skip=skip_connection, use_batchnorm=use_batchnorm)(x)\n",
    "        \n",
    "        hyper_col_name [i] = Conv2D(64,1,padding='same',activation=None)(x) \n",
    "        \n",
    "    x = SpatialDropout2D(0.25)(x)\n",
    "    \n",
    "    \n",
    "    ##------------Start of Hyper column --------------------------------------------------------\n",
    "    '''\n",
    "    hypercol3 = Concatenate(name='hypercol3')([x,   UpSampling2D(size=(2,   2))(hyper_col_name.get(3))])    \n",
    "    hypercol2 = Concatenate(name='hypercol2')([hypercol3, UpSampling2D(size=(4,   4))(hyper_col_name.get(2))]) \n",
    "    hypercol1 = Concatenate(name='hypercol1')([hypercol2, UpSampling2D(size=(8,   8))(hyper_col_name.get(1))])\n",
    "    hypercol0 = Concatenate(name='hypercol0')([hypercol1, UpSampling2D(size=(16, 16))(hyper_col_name.get(0))])\n",
    "\n",
    "    hypercol = Conv2D(64,  (3, 3), activation=None, padding=\"same\")(hypercol0)\n",
    "    #hypercol0 = Dropout(0.5)(hypercol)\n",
    "    \n",
    "    ##------------End of Hyper column --------------------------------------------------------\n",
    "    '''\n",
    "    hypercol4 = UpSampling2D(size=(1,   1))(x)\n",
    "    hypercol3 = UpSampling2D(size=(2,   2))(hyper_col_name.get(3))\n",
    "    hypercol2 = UpSampling2D(size=(4,   4))(hyper_col_name.get(2)) \n",
    "    hypercol1 = UpSampling2D(size=(8,   8))(hyper_col_name.get(1))\n",
    "    hypercol0 = UpSampling2D(size=(16, 16))(hyper_col_name.get(0)) \n",
    "    \n",
    "    hypercol = concatenate([hypercol0,hypercol1,hypercol2,hypercol3,hypercol4],axis=-1)\n",
    "    \n",
    "    hypercol = concatenate([hypercol0,hypercol1,hypercol2,hypercol3,hypercol4])\n",
    "    x = Conv2D(1, (3,3), padding='same', name='decode_final_conv')(hypercol)    \n",
    "    out_hyper_colx = Activation('sigmoid', name='final_decode_fuse')(x) #-From decoder\n",
    "    \n",
    "    side_fuse_layer1 = side_block_dict.get(0)\n",
    "    side_fuse_layer1 = GlobalAveragePooling2D(name='global_avg_pooling_1')(side_fuse_layer1)\n",
    "    side_fuse_layer1 = Dense(1 ,name='side_fuse_layer_1_dense')(side_fuse_layer1)\n",
    "    side_fuse_layer1 = Activation('sigmoid', name='side_fuse_layer1')(side_fuse_layer1)\n",
    "    \n",
    "    side_fuse_layer2 = Concatenate(name='side_fuse_layer2_start')([side_block_dict.get(0), side_block_dict.get(1)])\n",
    "    side_fuse_layer2 = GlobalAveragePooling2D(name='global_avg_pooling_2')(side_fuse_layer2)\n",
    "    side_fuse_layer2 = Concatenate(name='side_fuse_layer2_layer1')([side_fuse_layer1,side_fuse_layer2])\n",
    "    side_fuse_layer2 = Dense(1 ,name='side_fuse_layer_2_dense')(side_fuse_layer2)\n",
    "    side_fuse_layer2 = Activation('sigmoid', name='side_fuse_layer2')(side_fuse_layer2)\n",
    "    \n",
    "    side_fuse_layer3 = Concatenate(name='side_fuse_layer3_start')([side_block_dict.get(0), side_block_dict.get(1), side_block_dict.get(2)])\n",
    "    side_fuse_layer3 = GlobalAveragePooling2D(name='global_avg_pooling_3')(side_fuse_layer3)\n",
    "    side_fuse_layer3 = Concatenate(name='side_fuse_layer3_layer2_layer1')([side_fuse_layer1,side_fuse_layer2,side_fuse_layer3])\n",
    "    side_fuse_layer3 = Dense(1 ,name='side_fuse_layer_3_dense')(side_fuse_layer3)\n",
    "    side_fuse_layer3 = Activation('sigmoid', name='side_fuse_layer3')(side_fuse_layer3)\n",
    "    \n",
    "    side_fuse_layer4_int = Concatenate(name='side_fuse_layer4_int')([side_block_dict.get(0), side_block_dict.get(1), side_block_dict.get(2), side_block_dict.get(3)])\n",
    "    side_fuse_layer4 = GlobalAveragePooling2D(name='global_avg_pooling_4')(side_fuse_layer4_int)\n",
    "    side_fuse_layer4 = Concatenate(name='side_fuse_layer4_layer3_layer2_layer1')([side_fuse_layer1,side_fuse_layer2,side_fuse_layer3,side_fuse_layer4])\n",
    "    side_fuse_layer4 = Dense(1 ,name='side_fuse_layer_4_dense')(side_fuse_layer4)\n",
    "    side_fuse_layer4 = Activation('sigmoid', name='side_fuse_layer4')(side_fuse_layer4)\n",
    "    #side_fuse_layer = concatenate([side_block_dict.get(0), side_block_dict.get(1), side_block_dict.get(2), side_block_dict.get(3)])\n",
    "    #side_fuse_layer = side_block_dict.get(0)\n",
    "    \n",
    "    side_fuse_layer_int = Conv2D(64, (1,1), padding='same' ,activation=None)(side_fuse_layer4_int)\n",
    "\n",
    "    \n",
    "    fuse_overall_seg = concatenate([out_hyper_colx,side_fuse_layer_int]) # - Decoder + Encoder\n",
    "    fuse_overall_seg = Conv2D(1, (3,3), padding='same')(fuse_overall_seg)\n",
    "    fuse_overall_seg = Activation('sigmoid', name='Overall_fuse_conv')(fuse_overall_seg)\n",
    "    \n",
    "    model = Model(input_x, output = [side_fuse_layer4,out_hyper_colx,fuse_overall_seg])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import get_file\n",
    "\n",
    "weights_collection = [\n",
    " {\n",
    "        'model': 'resnet34',\n",
    "        'dataset': 'imagenet',\n",
    "        'classes': 1000,\n",
    "        'include_top': False,\n",
    "        'url': 'resnet34_imagenet_1000_no_top.h5',\n",
    "        'name': 'resnet34_imagenet_1000_no_top.h5',\n",
    "        'md5': '8caaa0ad39d927cb8ba5385bf945d582',\n",
    " }\n",
    "\n",
    "]\n",
    "\n",
    "#weights_collections = load_model('resnet34_imagenet_1000_no_top.h5')\n",
    "\n",
    "\n",
    "def find_weights(weights_collection, model_name, dataset, include_top):\n",
    "    w = list(filter(lambda x: x['model'] == model_name, weights_collection))\n",
    "    w = list(filter(lambda x: x['dataset'] == dataset, w))\n",
    "    w = list(filter(lambda x: x['include_top'] == include_top, w))\n",
    "    return w\n",
    "\n",
    "\n",
    "def load_model_weights(weights_collection, model, dataset, classes, include_top):\n",
    "    weights = find_weights(weights_collection, model.name, dataset, include_top)\n",
    "\n",
    "    if weights:\n",
    "        weights = weights[0]\n",
    "\n",
    "        if include_top and weights['classes'] != classes:\n",
    "            raise ValueError('If using `weights` and `include_top`'\n",
    "                             ' as true, `classes` should be {}'.format(weights['classes']))\n",
    "\n",
    "        weights_path = get_file(weights['name'],\n",
    "                                weights['url'],\n",
    "                                cache_subdir='models',\n",
    "                                md5_hash=weights['md5'])\n",
    "\n",
    "        model.load_weights(weights_path)\n",
    "\n",
    "    else:\n",
    "        raise ValueError('There is no weights for such configuration: ' +\n",
    "                         'model = {}, dataset = {}, '.format(model.name, dataset) +\n",
    "                         'classes = {}, include_top = {}.'.format(classes, include_top))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_params(**params):\n",
    "    default_conv_params = {\n",
    "        'kernel_initializer': 'glorot_uniform',\n",
    "        'use_bias': False,\n",
    "        'padding': 'valid',\n",
    "    }\n",
    "    default_conv_params.update(params)\n",
    "    return default_conv_params\n",
    "\n",
    "\n",
    "def get_bn_params(**params):\n",
    "    default_bn_params = {\n",
    "        'axis': 3,\n",
    "        'momentum': 0.99,\n",
    "        'epsilon': 2e-5,\n",
    "        'center': True,\n",
    "        'scale': True,\n",
    "    }\n",
    "    default_bn_params.update(params)\n",
    "    return default_bn_params\n",
    "\n",
    "def handle_block_names(stage, block):\n",
    "    name_base = 'stage{}_unit{}_'.format(stage + 1, block + 1)\n",
    "    conv_name = name_base + 'conv'\n",
    "    bn_name = name_base + 'bn'\n",
    "    relu_name = name_base + 'relu'\n",
    "    sc_name = name_base + 'sc'\n",
    "    return conv_name, bn_name, relu_name, sc_name\n",
    "\n",
    "\n",
    "def basic_identity_block(filters, stage, block):\n",
    "    \"\"\"The identity block is the block that has no conv layer at shortcut.\n",
    "    # Arguments\n",
    "        kernel_size: default 3, the kernel size of\n",
    "            middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    \"\"\"\n",
    "\n",
    "    def layer(input_tensor):\n",
    "        conv_params = get_conv_params()\n",
    "        bn_params = get_bn_params()\n",
    "        conv_name, bn_name, relu_name, sc_name = handle_block_names(stage, block)\n",
    "\n",
    "        x = BatchNormalization(name=bn_name + '1', **bn_params)(input_tensor)\n",
    "        x = Activation('relu', name=relu_name + '1')(x)\n",
    "        x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "        x = Conv2D(filters, (3, 3), name=conv_name + '1', **conv_params)(x)\n",
    "\n",
    "        x = BatchNormalization(name=bn_name + '2', **bn_params)(x)\n",
    "        x = Activation('relu', name=relu_name + '2')(x)\n",
    "        x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "        x = Conv2D(filters, (3, 3), name=conv_name + '2', **conv_params)(x)\n",
    "\n",
    "        x = Add()([x, input_tensor])\n",
    "        return x\n",
    "\n",
    "    return layer\n",
    "\n",
    "\n",
    "def basic_conv_block(filters, stage, block, strides=(2, 2)):\n",
    "    \"\"\"The identity block is the block that has no conv layer at shortcut.\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of\n",
    "            middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    \"\"\"\n",
    "\n",
    "    def layer(input_tensor):\n",
    "        conv_params = get_conv_params()\n",
    "        bn_params = get_bn_params()\n",
    "        conv_name, bn_name, relu_name, sc_name = handle_block_names(stage, block)\n",
    "\n",
    "        x = BatchNormalization(name=bn_name + '1', **bn_params)(input_tensor)\n",
    "        x = Activation('relu', name=relu_name + '1')(x)\n",
    "        shortcut = x\n",
    "        x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "        x = Conv2D(filters, (3, 3), strides=strides, name=conv_name + '1', **conv_params)(x)\n",
    "\n",
    "        x = BatchNormalization(name=bn_name + '2', **bn_params)(x)\n",
    "        x = Activation('relu', name=relu_name + '2')(x)\n",
    "        x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "        x = Conv2D(filters, (3, 3), name=conv_name + '2', **conv_params)(x)\n",
    "\n",
    "        shortcut = Conv2D(filters, (1, 1), name=sc_name, strides=strides, **conv_params)(shortcut)\n",
    "        x = Add()([x, shortcut])\n",
    "        return x\n",
    "\n",
    "    return layer\n",
    "\n",
    "\n",
    "def conv_block(filters, stage, block, strides=(2, 2)):\n",
    "    \"\"\"The identity block is the block that has no conv layer at shortcut.\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of\n",
    "            middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    \"\"\"\n",
    "\n",
    "    def layer(input_tensor):\n",
    "        conv_params = get_conv_params()\n",
    "        bn_params = get_bn_params()\n",
    "        conv_name, bn_name, relu_name, sc_name = handle_block_names(stage, block)\n",
    "\n",
    "        x = BatchNormalization(name=bn_name + '1', **bn_params)(input_tensor)\n",
    "        x = Activation('relu', name=relu_name + '1')(x)\n",
    "        shortcut = x\n",
    "        x = Conv2D(filters, (1, 1), name=conv_name + '1', **conv_params)(x)\n",
    "\n",
    "        x = BatchNormalization(name=bn_name + '2', **bn_params)(x)\n",
    "        x = Activation('relu', name=relu_name + '2')(x)\n",
    "        x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "        x = Conv2D(filters, (3, 3), strides=strides, name=conv_name + '2', **conv_params)(x)\n",
    "\n",
    "        x = BatchNormalization(name=bn_name + '3', **bn_params)(x)\n",
    "        x = Activation('relu', name=relu_name + '3')(x)\n",
    "        x = Conv2D(filters*4, (1, 1), name=conv_name + '3', **conv_params)(x)\n",
    "        \n",
    "        x = sc_se_block(x, filters*4 , ratio=16)\n",
    "        \n",
    "        shortcut = Conv2D(filters*4, (1, 1), name=sc_name, strides=strides, **conv_params)(shortcut)\n",
    "        x = Add()([x, shortcut])\n",
    "        return x\n",
    "\n",
    "    return layer\n",
    "\n",
    "\n",
    "def identity_block(filters, stage, block):\n",
    "    \"\"\"The identity block is the block that has no conv layer at shortcut.\n",
    "    # Arguments\n",
    "        kernel_size: default 3, the kernel size of\n",
    "            middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    \"\"\"\n",
    "\n",
    "    def layer(input_tensor):\n",
    "        conv_params = get_conv_params()\n",
    "        bn_params = get_bn_params()\n",
    "        conv_name, bn_name, relu_name, sc_name = handle_block_names(stage, block)\n",
    "\n",
    "        x = BatchNormalization(name=bn_name + '1', **bn_params)(input_tensor)\n",
    "        x = Activation('relu', name=relu_name + '1')(x)\n",
    "        x = Conv2D(filters, (1, 1), name=conv_name + '1', **conv_params)(x)\n",
    "\n",
    "        x = BatchNormalization(name=bn_name + '2', **bn_params)(x)\n",
    "        x = Activation('relu', name=relu_name + '2')(x)\n",
    "        x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "        x = Conv2D(filters, (3, 3), name=conv_name + '2', **conv_params)(x)\n",
    "\n",
    "        x = BatchNormalization(name=bn_name + '3', **bn_params)(x)\n",
    "        x = Activation('relu', name=relu_name + '3')(x)\n",
    "        x = Conv2D(filters*4, (1, 1), name=conv_name + '3', **conv_params)(x)\n",
    "\n",
    "        x = Add()([x, input_tensor])\n",
    "        return x\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.imagenet_utils import _obtain_input_shape\n",
    "def build_resnet(\n",
    "     repetitions=(2, 2, 2, 2),\n",
    "     include_top=True,\n",
    "     input_tensor=None,\n",
    "     input_shape=None,\n",
    "     classes=1000,\n",
    "     block_type='usual'):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    \n",
    "    # Determine proper input shape\n",
    "    input_shape = _obtain_input_shape(input_shape,\n",
    "                                      default_size=128,\n",
    "                                      min_size=128,\n",
    "                                      data_format='channels_last',\n",
    "                                      require_flatten=include_top)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape, name='data')\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "    \n",
    "    # get parameters for model layers\n",
    "    no_scale_bn_params = get_bn_params(scale=False)\n",
    "    bn_params = get_bn_params()\n",
    "    conv_params = get_conv_params()\n",
    "    init_filters = 64\n",
    "\n",
    "    if block_type == 'basic':\n",
    "        conv_block = basic_conv_block\n",
    "        identity_block = basic_identity_block\n",
    "    else:\n",
    "        conv_block = usual_conv_block\n",
    "        identity_block = usual_identity_block\n",
    "    \n",
    "    # resnet bottom\n",
    "    x = BatchNormalization(name='bn_data', **no_scale_bn_params)(img_input)\n",
    "    x = ZeroPadding2D(padding=(3, 3))(x)\n",
    "    x = Conv2D(init_filters, (7, 7), strides=(2, 2), name='conv0', **conv_params)(x)\n",
    "    x = BatchNormalization(name='bn0', **bn_params)(x)\n",
    "    x = Activation('relu', name='relu0')(x)\n",
    "   \n",
    "    x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='valid', name='pooling0')(x) \n",
    "    \n",
    "    k=1\n",
    "    # resnet body\n",
    "    for stage, rep in enumerate(repetitions):\n",
    "        for block in range(rep):\n",
    "            \n",
    "            filters = init_filters * (2**stage)\n",
    "            \n",
    "            # first block of first stage without strides because we have maxpooling before\n",
    "            if block == 0 and stage == 0:\n",
    "                x = conv_block(filters, stage, block, strides=(1, 1))(x)\n",
    "                \n",
    "            elif block == 0:\n",
    "                x = conv_block(filters, stage, block, strides=(2, 2))(x)\n",
    "                \n",
    "            else:\n",
    "                x = identity_block(filters, stage, block)(x)\n",
    "        \n",
    "        k *=2\n",
    "        \n",
    "        side_block_dict[stage]= side_branch(x, k)\n",
    "        \n",
    "    x = BatchNormalization(name='bn1', **bn_params)(x)\n",
    "    x = Activation('relu', name='relu1')(x)\n",
    "    \n",
    "    #side_block_dict[0] = side_branch(x,16)\n",
    "    \n",
    "    #side_block = Dense(1, name='fc1')(x)\n",
    "    #side_block = Activation('sigmoid', name='softmax')(x)\n",
    "    \n",
    "    # resnet top\n",
    "    if include_top:\n",
    "        x = GlobalAveragePooling2D(name='pool1')(x)\n",
    "        x = Dense(classes, name='fc1')(x)\n",
    "        x = Activation('softmax', name='softmax')(x)\n",
    "\n",
    "    # Ensure that the model takes into account any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "    \n",
    "       \n",
    "    # Create model.\n",
    "    model = Model(inputs, x)\n",
    "    #model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n",
    "def ResNet34(input_shape, input_tensor=None, weights=None, classes=1000, include_top=True):\n",
    "    model = build_resnet(input_tensor=input_tensor,\n",
    "                         input_shape=input_shape,\n",
    "                         repetitions=(3, 4, 6, 3),\n",
    "                         classes=classes,\n",
    "                         include_top=include_top,\n",
    "                         block_type='basic')\n",
    "    model.name = 'resnet34'\n",
    "\n",
    "    if weights:\n",
    "        load_model_weights(weights_collection, model, weights, classes, include_top)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbones = {\n",
    "    \"resnet34\": ResNet34,\n",
    "}\n",
    "def get_backbone(name, *args, **kwargs):\n",
    "    return backbones[name](*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_model(model):\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SKIP_CONNECTIONS = {\n",
    "    'resnet34': ('stage4_unit1_relu1', 'stage3_unit1_relu1', 'stage2_unit1_relu1', 'relu0'),\n",
    "}\n",
    "\n",
    "def Unet(backbone_name='resnet34',\n",
    "         input_shape=(None, None, 4),\n",
    "         input_tensor=None,\n",
    "         encoder_weights='imagenet',\n",
    "         freeze_encoder=False,\n",
    "         skip_connections='default',\n",
    "         decoder_block_type='upsampling',\n",
    "         decoder_filters=(256,128,64,32,16),\n",
    "         decoder_use_batchnorm=True,\n",
    "         n_upsample_blocks=5,\n",
    "         upsample_rates=(2,2,2,2,2),\n",
    "         classes=1,\n",
    "         activation='sigmoid'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        backbone_name: (str) look at list of available backbones.\n",
    "        input_shape:  (tuple) dimensions of input data (H, W, C)\n",
    "        input_tensor: keras tensor\n",
    "        encoder_weights: one of `None` (random initialization), 'imagenet' (pre-training on ImageNet)\n",
    "        freeze_encoder: (bool) Set encoder layers weights as non-trainable. Useful for fine-tuning\n",
    "        skip_connections: if 'default' is used take default skip connections,\n",
    "            else provide a list of layer numbers or names starting from top of model\n",
    "        decoder_block_type: (str) one of 'upsampling' and 'transpose' (look at blocks.py)\n",
    "        decoder_filters: (int) number of convolution filters in last upsample block\n",
    "        decoder_use_batchnorm: (bool) if True add batch normalisation layer between `Conv2D` ad `Activation` layers\n",
    "        n_upsample_blocks: (int) a number of upsampling blocks\n",
    "        upsample_rates: (tuple of int) upsampling rates decoder blocks\n",
    "        classes: (int) a number of classes for output\n",
    "        activation: (str) one of keras activations\n",
    "    Returns:\n",
    "        keras.models.Model instance\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    backbone = get_backbone(backbone_name,\n",
    "                            input_shape=input_shape,\n",
    "                            input_tensor=input_tensor,\n",
    "                            weights=encoder_weights,\n",
    "                            include_top=False)\n",
    "\n",
    "    if skip_connections == 'default':\n",
    "        skip_connections = DEFAULT_SKIP_CONNECTIONS[backbone_name]\n",
    "\n",
    "    model = build_unet(backbone,\n",
    "                       classes,\n",
    "                       skip_connections,\n",
    "                       decoder_filters=decoder_filters,\n",
    "                       block_type=decoder_block_type,\n",
    "                       activation=activation,\n",
    "                       n_upsample_blocks=n_upsample_blocks,\n",
    "                       upsample_rates=upsample_rates,\n",
    "                       use_batchnorm=decoder_use_batchnorm)\n",
    "\n",
    "    # lock encoder weights for fine-tuning\n",
    "    if freeze_encoder:\n",
    "        freeze_model(backbone)\n",
    "    \n",
    "    #output = [side_fuse_layer1,side_fuse_layer2,side_fuse_layer3,side_fuse_layer4,side_fuse_layer,\n",
    "    #                               hypercol3,hyercol2,hypercol1,hypercol0,out_hyper_colx,\n",
    "    #                               fuse_overall_seg])\n",
    "    \n",
    "    model.name = 'u-{}'.format(backbone_name)\n",
    "    model.compile(optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08), \n",
    "                  loss = {\n",
    "                          'side_fuse_layer4': 'binary_crossentropy',\n",
    "                          'final_decode_fuse':my_crossentropy_non_empty,                        \n",
    "                        'Overall_fuse_conv': dice_loss\n",
    "                        },\n",
    "                  loss_weights ={\n",
    "                          'side_fuse_layer4':  0.05,\n",
    "                          'final_decode_fuse': 0.1,                        \n",
    "                        'Overall_fuse_conv': 1\n",
    "                        },\n",
    "                  metrics = {\n",
    "                          'side_fuse_layer4':  'accuracy',\n",
    "                          'final_decode_fuse': my_iou_metric_non_zero,                        \n",
    "                        'Overall_fuse_conv': my_iou_metric                      \n",
    "                        })\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sourav/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:96: UserWarning: Update your `Model` call to the Keras 2 API: `Model(Tensor(\"da..., outputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "# Setting up for training\n",
    "chnl=3\n",
    "n_features = 1\n",
    "model = Unet(input_shape=(img_size_target, img_size_target, 3))\n",
    "early_stopping = EarlyStopping(monitor='final_decode_fuse_my_iou_metric_non_zero',mode = 'max',patience=10, verbose=1)\n",
    "model_checkpoint = ModelCheckpoint(\"./keras.model\", monitor='final_decode_fuse_my_iou_metric_non_zero',mode = 'max',  save_best_only=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='final_decode_fuse_my_iou_metric_non_zero',mode = 'max',factor=0.1, patience=5, min_lr=0.00000001, verbose=1)\n",
    "#lr = 0.0005\n",
    "#clr_triangular = CyclicLR(mode='exp_range', gamma=0.99994)\n",
    "#clr_triangular._reset(new_base_lr=lr, new_max_lr= lr * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7180 samples, validate on 399 samples\n",
      "Epoch 1/100\n",
      "7180/7180 [==============================] - 47s 7ms/step - loss: 0.3970 - side_fuse_layer4_loss: 0.8761 - final_decode_fuse_loss: 0.0640 - Overall_fuse_conv_loss: 0.3468 - side_fuse_layer4_acc: 0.4563 - final_decode_fuse_my_iou_metric_non_zero: 0.4698 - Overall_fuse_conv_my_iou_metric: 0.3221 - val_loss: 0.2660 - val_side_fuse_layer4_loss: 0.5201 - val_final_decode_fuse_loss: 0.0602 - val_Overall_fuse_conv_loss: 0.2340 - val_side_fuse_layer4_acc: 0.6917 - val_final_decode_fuse_my_iou_metric_non_zero: 0.5128 - val_Overall_fuse_conv_my_iou_metric: 0.4937\n",
      "\n",
      "Epoch 00001: final_decode_fuse_my_iou_metric_non_zero improved from -inf to 0.46982, saving model to ./keras.model\n",
      "Epoch 2/100\n",
      "7180/7180 [==============================] - 39s 5ms/step - loss: 0.2342 - side_fuse_layer4_loss: 0.4675 - final_decode_fuse_loss: 0.0607 - Overall_fuse_conv_loss: 0.2047 - side_fuse_layer4_acc: 0.7763 - final_decode_fuse_my_iou_metric_non_zero: 0.5343 - Overall_fuse_conv_my_iou_metric: 0.5701 - val_loss: 0.3481 - val_side_fuse_layer4_loss: 0.5477 - val_final_decode_fuse_loss: 0.0619 - val_Overall_fuse_conv_loss: 0.3145 - val_side_fuse_layer4_acc: 0.7920 - val_final_decode_fuse_my_iou_metric_non_zero: 0.3820 - val_Overall_fuse_conv_my_iou_metric: 0.4288\n",
      "\n",
      "Epoch 00002: final_decode_fuse_my_iou_metric_non_zero improved from 0.46982 to 0.53430, saving model to ./keras.model\n",
      "Epoch 3/100\n",
      "7180/7180 [==============================] - 41s 6ms/step - loss: 0.2049 - side_fuse_layer4_loss: 0.4031 - final_decode_fuse_loss: 0.0582 - Overall_fuse_conv_loss: 0.1789 - side_fuse_layer4_acc: 0.8238 - final_decode_fuse_my_iou_metric_non_zero: 0.4958 - Overall_fuse_conv_my_iou_metric: 0.6044 - val_loss: 0.2913 - val_side_fuse_layer4_loss: 0.5470 - val_final_decode_fuse_loss: 0.0609 - val_Overall_fuse_conv_loss: 0.2579 - val_side_fuse_layer4_acc: 0.7820 - val_final_decode_fuse_my_iou_metric_non_zero: 0.3398 - val_Overall_fuse_conv_my_iou_metric: 0.4910\n",
      "\n",
      "Epoch 00003: final_decode_fuse_my_iou_metric_non_zero did not improve from 0.53430\n",
      "Epoch 4/100\n",
      "7180/7180 [==============================] - 31s 4ms/step - loss: 0.1975 - side_fuse_layer4_loss: 0.3839 - final_decode_fuse_loss: 0.0580 - Overall_fuse_conv_loss: 0.1725 - side_fuse_layer4_acc: 0.8316 - final_decode_fuse_my_iou_metric_non_zero: 0.4802 - Overall_fuse_conv_my_iou_metric: 0.6165 - val_loss: 0.2561 - val_side_fuse_layer4_loss: 0.5355 - val_final_decode_fuse_loss: 0.0593 - val_Overall_fuse_conv_loss: 0.2234 - val_side_fuse_layer4_acc: 0.8296 - val_final_decode_fuse_my_iou_metric_non_zero: 0.5381 - val_Overall_fuse_conv_my_iou_metric: 0.4927\n",
      "\n",
      "Epoch 00004: final_decode_fuse_my_iou_metric_non_zero did not improve from 0.53430\n",
      "Epoch 5/100\n",
      "7180/7180 [==============================] - 54s 8ms/step - loss: 0.1776 - side_fuse_layer4_loss: 0.3448 - final_decode_fuse_loss: 0.0568 - Overall_fuse_conv_loss: 0.1547 - side_fuse_layer4_acc: 0.8497 - final_decode_fuse_my_iou_metric_non_zero: 0.5136 - Overall_fuse_conv_my_iou_metric: 0.6443 - val_loss: 0.1878 - val_side_fuse_layer4_loss: 0.3729 - val_final_decode_fuse_loss: 0.0546 - val_Overall_fuse_conv_loss: 0.1637 - val_side_fuse_layer4_acc: 0.8346 - val_final_decode_fuse_my_iou_metric_non_zero: 0.6153 - val_Overall_fuse_conv_my_iou_metric: 0.6118\n",
      "\n",
      "Epoch 00005: final_decode_fuse_my_iou_metric_non_zero did not improve from 0.53430\n",
      "Epoch 6/100\n",
      "7180/7180 [==============================] - 37s 5ms/step - loss: 0.1764 - side_fuse_layer4_loss: 0.3284 - final_decode_fuse_loss: 0.0563 - Overall_fuse_conv_loss: 0.1544 - side_fuse_layer4_acc: 0.8614 - final_decode_fuse_my_iou_metric_non_zero: 0.5217 - Overall_fuse_conv_my_iou_metric: 0.6615 - val_loss: 0.2674 - val_side_fuse_layer4_loss: 0.6900 - val_final_decode_fuse_loss: 0.0600 - val_Overall_fuse_conv_loss: 0.2269 - val_side_fuse_layer4_acc: 0.7494 - val_final_decode_fuse_my_iou_metric_non_zero: 0.6383 - val_Overall_fuse_conv_my_iou_metric: 0.6311\n",
      "\n",
      "Epoch 00006: final_decode_fuse_my_iou_metric_non_zero did not improve from 0.53430\n",
      "Epoch 7/100\n",
      "7180/7180 [==============================] - 20s 3ms/step - loss: 0.1834 - side_fuse_layer4_loss: 0.3378 - final_decode_fuse_loss: 0.0567 - Overall_fuse_conv_loss: 0.1609 - side_fuse_layer4_acc: 0.8554 - final_decode_fuse_my_iou_metric_non_zero: 0.4931 - Overall_fuse_conv_my_iou_metric: 0.6526 - val_loss: 0.1755 - val_side_fuse_layer4_loss: 0.4489 - val_final_decode_fuse_loss: 0.0537 - val_Overall_fuse_conv_loss: 0.1477 - val_side_fuse_layer4_acc: 0.8421 - val_final_decode_fuse_my_iou_metric_non_zero: 0.6108 - val_Overall_fuse_conv_my_iou_metric: 0.6972\n",
      "\n",
      "Epoch 00007: final_decode_fuse_my_iou_metric_non_zero did not improve from 0.53430\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 8/100\n",
      "7180/7180 [==============================] - 39s 5ms/step - loss: 0.1549 - side_fuse_layer4_loss: 0.3002 - final_decode_fuse_loss: 0.0550 - Overall_fuse_conv_loss: 0.1344 - side_fuse_layer4_acc: 0.8734 - final_decode_fuse_my_iou_metric_non_zero: 0.5463 - Overall_fuse_conv_my_iou_metric: 0.6683 - val_loss: 0.1499 - val_side_fuse_layer4_loss: 0.2898 - val_final_decode_fuse_loss: 0.0521 - val_Overall_fuse_conv_loss: 0.1302 - val_side_fuse_layer4_acc: 0.8697 - val_final_decode_fuse_my_iou_metric_non_zero: 0.6268 - val_Overall_fuse_conv_my_iou_metric: 0.7429\n",
      "\n",
      "Epoch 00008: final_decode_fuse_my_iou_metric_non_zero improved from 0.53430 to 0.54627, saving model to ./keras.model\n",
      "Epoch 9/100\n",
      "7180/7180 [==============================] - 41s 6ms/step - loss: 0.1457 - side_fuse_layer4_loss: 0.2900 - final_decode_fuse_loss: 0.0543 - Overall_fuse_conv_loss: 0.1257 - side_fuse_layer4_acc: 0.8753 - final_decode_fuse_my_iou_metric_non_zero: 0.5423 - Overall_fuse_conv_my_iou_metric: 0.7015 - val_loss: 0.1480 - val_side_fuse_layer4_loss: 0.2823 - val_final_decode_fuse_loss: 0.0519 - val_Overall_fuse_conv_loss: 0.1287 - val_side_fuse_layer4_acc: 0.8697 - val_final_decode_fuse_my_iou_metric_non_zero: 0.6246 - val_Overall_fuse_conv_my_iou_metric: 0.7501\n",
      "\n",
      "Epoch 00009: final_decode_fuse_my_iou_metric_non_zero did not improve from 0.54627\n",
      "Epoch 10/100\n",
      "7180/7180 [==============================] - 45s 6ms/step - loss: 0.1451 - side_fuse_layer4_loss: 0.2838 - final_decode_fuse_loss: 0.0542 - Overall_fuse_conv_loss: 0.1255 - side_fuse_layer4_acc: 0.8779 - final_decode_fuse_my_iou_metric_non_zero: 0.4988 - Overall_fuse_conv_my_iou_metric: 0.7080 - val_loss: 0.1527 - val_side_fuse_layer4_loss: 0.2802 - val_final_decode_fuse_loss: 0.0520 - val_Overall_fuse_conv_loss: 0.1335 - val_side_fuse_layer4_acc: 0.8747 - val_final_decode_fuse_my_iou_metric_non_zero: 0.5902 - val_Overall_fuse_conv_my_iou_metric: 0.7459\n",
      "\n",
      "Epoch 00010: final_decode_fuse_my_iou_metric_non_zero did not improve from 0.54627\n",
      "Epoch 11/100\n",
      "7180/7180 [==============================] - 36s 5ms/step - loss: 0.1434 - side_fuse_layer4_loss: 0.2822 - final_decode_fuse_loss: 0.0541 - Overall_fuse_conv_loss: 0.1238 - side_fuse_layer4_acc: 0.8804 - final_decode_fuse_my_iou_metric_non_zero: 0.5146 - Overall_fuse_conv_my_iou_metric: 0.7089 - val_loss: 0.1468 - val_side_fuse_layer4_loss: 0.2849 - val_final_decode_fuse_loss: 0.0515 - val_Overall_fuse_conv_loss: 0.1274 - val_side_fuse_layer4_acc: 0.8747 - val_final_decode_fuse_my_iou_metric_non_zero: 0.5822 - val_Overall_fuse_conv_my_iou_metric: 0.7429\n",
      "\n",
      "Epoch 00011: final_decode_fuse_my_iou_metric_non_zero did not improve from 0.54627\n",
      "Epoch 12/100\n",
      "7180/7180 [==============================] - 37s 5ms/step - loss: 0.1401 - side_fuse_layer4_loss: 0.2802 - final_decode_fuse_loss: 0.0542 - Overall_fuse_conv_loss: 0.1207 - side_fuse_layer4_acc: 0.8804 - final_decode_fuse_my_iou_metric_non_zero: 0.5072 - Overall_fuse_conv_my_iou_metric: 0.7112 - val_loss: 0.1584 - val_side_fuse_layer4_loss: 0.2931 - val_final_decode_fuse_loss: 0.0515 - val_Overall_fuse_conv_loss: 0.1386 - val_side_fuse_layer4_acc: 0.8596 - val_final_decode_fuse_my_iou_metric_non_zero: 0.5421 - val_Overall_fuse_conv_my_iou_metric: 0.7378\n",
      "\n",
      "Epoch 00012: final_decode_fuse_my_iou_metric_non_zero did not improve from 0.54627\n",
      "Epoch 13/100\n",
      "7180/7180 [==============================] - 38s 5ms/step - loss: 0.1332 - side_fuse_layer4_loss: 0.2730 - final_decode_fuse_loss: 0.0538 - Overall_fuse_conv_loss: 0.1142 - side_fuse_layer4_acc: 0.8855 - final_decode_fuse_my_iou_metric_non_zero: 0.4994 - Overall_fuse_conv_my_iou_metric: 0.7186 - val_loss: 0.1478 - val_side_fuse_layer4_loss: 0.2790 - val_final_decode_fuse_loss: 0.0512 - val_Overall_fuse_conv_loss: 0.1287 - val_side_fuse_layer4_acc: 0.8722 - val_final_decode_fuse_my_iou_metric_non_zero: 0.5386 - val_Overall_fuse_conv_my_iou_metric: 0.7396\n",
      "\n",
      "Epoch 00013: final_decode_fuse_my_iou_metric_non_zero did not improve from 0.54627\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 14/100\n",
      "7180/7180 [==============================] - 25s 3ms/step - loss: 0.1292 - side_fuse_layer4_loss: 0.2734 - final_decode_fuse_loss: 0.0535 - Overall_fuse_conv_loss: 0.1102 - side_fuse_layer4_acc: 0.8809 - final_decode_fuse_my_iou_metric_non_zero: 0.5011 - Overall_fuse_conv_my_iou_metric: 0.7239 - val_loss: 0.1437 - val_side_fuse_layer4_loss: 0.2713 - val_final_decode_fuse_loss: 0.0511 - val_Overall_fuse_conv_loss: 0.1251 - val_side_fuse_layer4_acc: 0.8772 - val_final_decode_fuse_my_iou_metric_non_zero: 0.5376 - val_Overall_fuse_conv_my_iou_metric: 0.7376\n",
      "\n",
      "Epoch 00014: final_decode_fuse_my_iou_metric_non_zero did not improve from 0.54627\n",
      "Epoch 15/100\n",
      "7180/7180 [==============================] - 40s 6ms/step - loss: 0.1310 - side_fuse_layer4_loss: 0.2692 - final_decode_fuse_loss: 0.0535 - Overall_fuse_conv_loss: 0.1122 - side_fuse_layer4_acc: 0.8836 - final_decode_fuse_my_iou_metric_non_zero: 0.5082 - Overall_fuse_conv_my_iou_metric: 0.7250 - val_loss: 0.1427 - val_side_fuse_layer4_loss: 0.2678 - val_final_decode_fuse_loss: 0.0511 - val_Overall_fuse_conv_loss: 0.1242 - val_side_fuse_layer4_acc: 0.8797 - val_final_decode_fuse_my_iou_metric_non_zero: 0.5434 - val_Overall_fuse_conv_my_iou_metric: 0.7386\n",
      "\n",
      "Epoch 00015: final_decode_fuse_my_iou_metric_non_zero did not improve from 0.54627\n",
      "Epoch 16/100\n",
      "7180/7180 [==============================] - 44s 6ms/step - loss: 0.1297 - side_fuse_layer4_loss: 0.2653 - final_decode_fuse_loss: 0.0534 - Overall_fuse_conv_loss: 0.1111 - side_fuse_layer4_acc: 0.8857 - final_decode_fuse_my_iou_metric_non_zero: 0.5137 - Overall_fuse_conv_my_iou_metric: 0.7253 - val_loss: 0.1422 - val_side_fuse_layer4_loss: 0.2665 - val_final_decode_fuse_loss: 0.0511 - val_Overall_fuse_conv_loss: 0.1237 - val_side_fuse_layer4_acc: 0.8797 - val_final_decode_fuse_my_iou_metric_non_zero: 0.5466 - val_Overall_fuse_conv_my_iou_metric: 0.7396\n",
      "\n",
      "Epoch 00016: final_decode_fuse_my_iou_metric_non_zero did not improve from 0.54627\n",
      "Epoch 17/100\n",
      "7180/7180 [==============================] - 33s 5ms/step - loss: 0.1315 - side_fuse_layer4_loss: 0.2689 - final_decode_fuse_loss: 0.0534 - Overall_fuse_conv_loss: 0.1127 - side_fuse_layer4_acc: 0.8844 - final_decode_fuse_my_iou_metric_non_zero: 0.5135 - Overall_fuse_conv_my_iou_metric: 0.7250 - val_loss: 0.1410 - val_side_fuse_layer4_loss: 0.2668 - val_final_decode_fuse_loss: 0.0511 - val_Overall_fuse_conv_loss: 0.1225 - val_side_fuse_layer4_acc: 0.8747 - val_final_decode_fuse_my_iou_metric_non_zero: 0.5449 - val_Overall_fuse_conv_my_iou_metric: 0.7431\n",
      "\n",
      "Epoch 00017: final_decode_fuse_my_iou_metric_non_zero did not improve from 0.54627\n",
      "Epoch 18/100\n",
      "7180/7180 [==============================] - 36s 5ms/step - loss: 0.1273 - side_fuse_layer4_loss: 0.2672 - final_decode_fuse_loss: 0.0534 - Overall_fuse_conv_loss: 0.1087 - side_fuse_layer4_acc: 0.8862 - final_decode_fuse_my_iou_metric_non_zero: 0.5131 - Overall_fuse_conv_my_iou_metric: 0.7255 - val_loss: 0.1418 - val_side_fuse_layer4_loss: 0.2674 - val_final_decode_fuse_loss: 0.0511 - val_Overall_fuse_conv_loss: 0.1234 - val_side_fuse_layer4_acc: 0.8772 - val_final_decode_fuse_my_iou_metric_non_zero: 0.5544 - val_Overall_fuse_conv_my_iou_metric: 0.7411\n",
      "\n",
      "Epoch 00018: final_decode_fuse_my_iou_metric_non_zero did not improve from 0.54627\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 19/100\n",
      "7180/7180 [==============================] - 36s 5ms/step - loss: 0.1320 - side_fuse_layer4_loss: 0.2647 - final_decode_fuse_loss: 0.0534 - Overall_fuse_conv_loss: 0.1134 - side_fuse_layer4_acc: 0.8891 - final_decode_fuse_my_iou_metric_non_zero: 0.5112 - Overall_fuse_conv_my_iou_metric: 0.7269 - val_loss: 0.1418 - val_side_fuse_layer4_loss: 0.2668 - val_final_decode_fuse_loss: 0.0511 - val_Overall_fuse_conv_loss: 0.1234 - val_side_fuse_layer4_acc: 0.8772 - val_final_decode_fuse_my_iou_metric_non_zero: 0.5451 - val_Overall_fuse_conv_my_iou_metric: 0.7404\n",
      "\n",
      "Epoch 00019: final_decode_fuse_my_iou_metric_non_zero did not improve from 0.54627\n",
      "Epoch 20/100\n",
      "7180/7180 [==============================] - 38s 5ms/step - loss: 0.1285 - side_fuse_layer4_loss: 0.2640 - final_decode_fuse_loss: 0.0535 - Overall_fuse_conv_loss: 0.1099 - side_fuse_layer4_acc: 0.8862 - final_decode_fuse_my_iou_metric_non_zero: 0.5113 - Overall_fuse_conv_my_iou_metric: 0.7287 - val_loss: 0.1422 - val_side_fuse_layer4_loss: 0.2669 - val_final_decode_fuse_loss: 0.0511 - val_Overall_fuse_conv_loss: 0.1237 - val_side_fuse_layer4_acc: 0.8772 - val_final_decode_fuse_my_iou_metric_non_zero: 0.5449 - val_Overall_fuse_conv_my_iou_metric: 0.7411\n",
      "\n",
      "Epoch 00020: final_decode_fuse_my_iou_metric_non_zero did not improve from 0.54627\n",
      "Epoch 21/100\n",
      "7180/7180 [==============================] - 40s 6ms/step - loss: 0.1293 - side_fuse_layer4_loss: 0.2658 - final_decode_fuse_loss: 0.0533 - Overall_fuse_conv_loss: 0.1106 - side_fuse_layer4_acc: 0.8868 - final_decode_fuse_my_iou_metric_non_zero: 0.5132 - Overall_fuse_conv_my_iou_metric: 0.7277 - val_loss: 0.1417 - val_side_fuse_layer4_loss: 0.2669 - val_final_decode_fuse_loss: 0.0510 - val_Overall_fuse_conv_loss: 0.1233 - val_side_fuse_layer4_acc: 0.8772 - val_final_decode_fuse_my_iou_metric_non_zero: 0.5459 - val_Overall_fuse_conv_my_iou_metric: 0.7416\n",
      "\n",
      "Epoch 00021: final_decode_fuse_my_iou_metric_non_zero did not improve from 0.54627\n",
      "Epoch 22/100\n",
      "7180/7180 [==============================] - 34s 5ms/step - loss: 0.1288 - side_fuse_layer4_loss: 0.2669 - final_decode_fuse_loss: 0.0534 - Overall_fuse_conv_loss: 0.1101 - side_fuse_layer4_acc: 0.8843 - final_decode_fuse_my_iou_metric_non_zero: 0.5143 - Overall_fuse_conv_my_iou_metric: 0.7291 - val_loss: 0.1414 - val_side_fuse_layer4_loss: 0.2666 - val_final_decode_fuse_loss: 0.0510 - val_Overall_fuse_conv_loss: 0.1229 - val_side_fuse_layer4_acc: 0.8747 - val_final_decode_fuse_my_iou_metric_non_zero: 0.5459 - val_Overall_fuse_conv_my_iou_metric: 0.7419\n",
      "\n",
      "Epoch 00022: final_decode_fuse_my_iou_metric_non_zero did not improve from 0.54627\n",
      "Epoch 23/100\n",
      "7180/7180 [==============================] - 56s 8ms/step - loss: 0.1292 - side_fuse_layer4_loss: 0.2683 - final_decode_fuse_loss: 0.0533 - Overall_fuse_conv_loss: 0.1105 - side_fuse_layer4_acc: 0.8857 - final_decode_fuse_my_iou_metric_non_zero: 0.5110 - Overall_fuse_conv_my_iou_metric: 0.7266 - val_loss: 0.1417 - val_side_fuse_layer4_loss: 0.2667 - val_final_decode_fuse_loss: 0.0511 - val_Overall_fuse_conv_loss: 0.1233 - val_side_fuse_layer4_acc: 0.8772 - val_final_decode_fuse_my_iou_metric_non_zero: 0.5459 - val_Overall_fuse_conv_my_iou_metric: 0.7409\n",
      "\n",
      "Epoch 00023: final_decode_fuse_my_iou_metric_non_zero did not improve from 0.54627\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 24/100\n",
      "7180/7180 [==============================] - 15s 2ms/step - loss: 0.1290 - side_fuse_layer4_loss: 0.2678 - final_decode_fuse_loss: 0.0535 - Overall_fuse_conv_loss: 0.1103 - side_fuse_layer4_acc: 0.8830 - final_decode_fuse_my_iou_metric_non_zero: 0.5087 - Overall_fuse_conv_my_iou_metric: 0.7274 - val_loss: 0.1416 - val_side_fuse_layer4_loss: 0.2667 - val_final_decode_fuse_loss: 0.0510 - val_Overall_fuse_conv_loss: 0.1231 - val_side_fuse_layer4_acc: 0.8797 - val_final_decode_fuse_my_iou_metric_non_zero: 0.5456 - val_Overall_fuse_conv_my_iou_metric: 0.7419\n",
      "\n",
      "Epoch 00024: final_decode_fuse_my_iou_metric_non_zero did not improve from 0.54627\n",
      "Epoch 25/100\n",
      "7180/7180 [==============================] - 38s 5ms/step - loss: 0.1298 - side_fuse_layer4_loss: 0.2678 - final_decode_fuse_loss: 0.0534 - Overall_fuse_conv_loss: 0.1111 - side_fuse_layer4_acc: 0.8844 - final_decode_fuse_my_iou_metric_non_zero: 0.5154 - Overall_fuse_conv_my_iou_metric: 0.7291 - val_loss: 0.1414 - val_side_fuse_layer4_loss: 0.2670 - val_final_decode_fuse_loss: 0.0510 - val_Overall_fuse_conv_loss: 0.1229 - val_side_fuse_layer4_acc: 0.8772 - val_final_decode_fuse_my_iou_metric_non_zero: 0.5456 - val_Overall_fuse_conv_my_iou_metric: 0.7416\n",
      "\n",
      "Epoch 00025: final_decode_fuse_my_iou_metric_non_zero did not improve from 0.54627\n",
      "Epoch 26/100\n",
      "7180/7180 [==============================] - 39s 5ms/step - loss: 0.1264 - side_fuse_layer4_loss: 0.2657 - final_decode_fuse_loss: 0.0533 - Overall_fuse_conv_loss: 0.1078 - side_fuse_layer4_acc: 0.8864 - final_decode_fuse_my_iou_metric_non_zero: 0.5101 - Overall_fuse_conv_my_iou_metric: 0.7297 - val_loss: 0.1416 - val_side_fuse_layer4_loss: 0.2671 - val_final_decode_fuse_loss: 0.0511 - val_Overall_fuse_conv_loss: 0.1231 - val_side_fuse_layer4_acc: 0.8772 - val_final_decode_fuse_my_iou_metric_non_zero: 0.5456 - val_Overall_fuse_conv_my_iou_metric: 0.7396\n",
      "\n",
      "Epoch 00026: final_decode_fuse_my_iou_metric_non_zero did not improve from 0.54627\n",
      "Epoch 27/100\n",
      "7180/7180 [==============================] - 40s 6ms/step - loss: 0.1286 - side_fuse_layer4_loss: 0.2678 - final_decode_fuse_loss: 0.0535 - Overall_fuse_conv_loss: 0.1098 - side_fuse_layer4_acc: 0.8852 - final_decode_fuse_my_iou_metric_non_zero: 0.5086 - Overall_fuse_conv_my_iou_metric: 0.7287 - val_loss: 0.1416 - val_side_fuse_layer4_loss: 0.2670 - val_final_decode_fuse_loss: 0.0511 - val_Overall_fuse_conv_loss: 0.1232 - val_side_fuse_layer4_acc: 0.8772 - val_final_decode_fuse_my_iou_metric_non_zero: 0.5456 - val_Overall_fuse_conv_my_iou_metric: 0.7424\n",
      "\n",
      "Epoch 00027: final_decode_fuse_my_iou_metric_non_zero did not improve from 0.54627\n",
      "Epoch 28/100\n",
      "7180/7180 [==============================] - 42s 6ms/step - loss: 0.1260 - side_fuse_layer4_loss: 0.2650 - final_decode_fuse_loss: 0.0533 - Overall_fuse_conv_loss: 0.1074 - side_fuse_layer4_acc: 0.8868 - final_decode_fuse_my_iou_metric_non_zero: 0.5132 - Overall_fuse_conv_my_iou_metric: 0.7283 - val_loss: 0.1420 - val_side_fuse_layer4_loss: 0.2670 - val_final_decode_fuse_loss: 0.0511 - val_Overall_fuse_conv_loss: 0.1235 - val_side_fuse_layer4_acc: 0.8747 - val_final_decode_fuse_my_iou_metric_non_zero: 0.5456 - val_Overall_fuse_conv_my_iou_metric: 0.7406\n",
      "\n",
      "Epoch 00028: final_decode_fuse_my_iou_metric_non_zero did not improve from 0.54627\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "Epoch 29/100\n",
      "7180/7180 [==============================] - 41s 6ms/step - loss: 0.1269 - side_fuse_layer4_loss: 0.2619 - final_decode_fuse_loss: 0.0534 - Overall_fuse_conv_loss: 0.1085 - side_fuse_layer4_acc: 0.8909 - final_decode_fuse_my_iou_metric_non_zero: 0.5105 - Overall_fuse_conv_my_iou_metric: 0.7306 - val_loss: 0.1419 - val_side_fuse_layer4_loss: 0.2667 - val_final_decode_fuse_loss: 0.0511 - val_Overall_fuse_conv_loss: 0.1235 - val_side_fuse_layer4_acc: 0.8772 - val_final_decode_fuse_my_iou_metric_non_zero: 0.5456 - val_Overall_fuse_conv_my_iou_metric: 0.7414\n",
      "\n",
      "Epoch 00029: final_decode_fuse_my_iou_metric_non_zero did not improve from 0.54627\n",
      "Epoch 30/100\n",
      "7180/7180 [==============================] - 41s 6ms/step - loss: 0.1291 - side_fuse_layer4_loss: 0.2668 - final_decode_fuse_loss: 0.0534 - Overall_fuse_conv_loss: 0.1104 - side_fuse_layer4_acc: 0.8870 - final_decode_fuse_my_iou_metric_non_zero: 0.5152 - Overall_fuse_conv_my_iou_metric: 0.7276 - val_loss: 0.1416 - val_side_fuse_layer4_loss: 0.2668 - val_final_decode_fuse_loss: 0.0511 - val_Overall_fuse_conv_loss: 0.1231 - val_side_fuse_layer4_acc: 0.8772 - val_final_decode_fuse_my_iou_metric_non_zero: 0.5459 - val_Overall_fuse_conv_my_iou_metric: 0.7414\n",
      "\n",
      "Epoch 00030: final_decode_fuse_my_iou_metric_non_zero did not improve from 0.54627\n",
      "Epoch 31/100\n",
      "7180/7180 [==============================] - 18s 3ms/step - loss: 0.1283 - side_fuse_layer4_loss: 0.2680 - final_decode_fuse_loss: 0.0532 - Overall_fuse_conv_loss: 0.1096 - side_fuse_layer4_acc: 0.8879 - final_decode_fuse_my_iou_metric_non_zero: 0.5141 - Overall_fuse_conv_my_iou_metric: 0.7310 - val_loss: 0.1414 - val_side_fuse_layer4_loss: 0.2666 - val_final_decode_fuse_loss: 0.0510 - val_Overall_fuse_conv_loss: 0.1230 - val_side_fuse_layer4_acc: 0.8772 - val_final_decode_fuse_my_iou_metric_non_zero: 0.5461 - val_Overall_fuse_conv_my_iou_metric: 0.7419\n",
      "\n",
      "Epoch 00031: final_decode_fuse_my_iou_metric_non_zero did not improve from 0.54627\n",
      "Epoch 32/100\n",
      "7180/7180 [==============================] - 39s 5ms/step - loss: 0.1270 - side_fuse_layer4_loss: 0.2667 - final_decode_fuse_loss: 0.0534 - Overall_fuse_conv_loss: 0.1084 - side_fuse_layer4_acc: 0.8877 - final_decode_fuse_my_iou_metric_non_zero: 0.5123 - Overall_fuse_conv_my_iou_metric: 0.7290 - val_loss: 0.1418 - val_side_fuse_layer4_loss: 0.2664 - val_final_decode_fuse_loss: 0.0510 - val_Overall_fuse_conv_loss: 0.1233 - val_side_fuse_layer4_acc: 0.8797 - val_final_decode_fuse_my_iou_metric_non_zero: 0.5459 - val_Overall_fuse_conv_my_iou_metric: 0.7411\n",
      "\n",
      "Epoch 00032: final_decode_fuse_my_iou_metric_non_zero did not improve from 0.54627\n",
      "Epoch 33/100\n",
      "7180/7180 [==============================] - 40s 6ms/step - loss: 0.1294 - side_fuse_layer4_loss: 0.2672 - final_decode_fuse_loss: 0.0534 - Overall_fuse_conv_loss: 0.1107 - side_fuse_layer4_acc: 0.8883 - final_decode_fuse_my_iou_metric_non_zero: 0.5168 - Overall_fuse_conv_my_iou_metric: 0.7312 - val_loss: 0.1420 - val_side_fuse_layer4_loss: 0.2669 - val_final_decode_fuse_loss: 0.0511 - val_Overall_fuse_conv_loss: 0.1235 - val_side_fuse_layer4_acc: 0.8772 - val_final_decode_fuse_my_iou_metric_non_zero: 0.5454 - val_Overall_fuse_conv_my_iou_metric: 0.7388\n",
      "\n",
      "Epoch 00033: final_decode_fuse_my_iou_metric_non_zero did not improve from 0.54627\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1e-08.\n",
      "Epoch 34/100\n",
      "7180/7180 [==============================] - 42s 6ms/step - loss: 0.1318 - side_fuse_layer4_loss: 0.2686 - final_decode_fuse_loss: 0.0534 - Overall_fuse_conv_loss: 0.1131 - side_fuse_layer4_acc: 0.8844 - final_decode_fuse_my_iou_metric_non_zero: 0.5138 - Overall_fuse_conv_my_iou_metric: 0.7296 - val_loss: 0.1416 - val_side_fuse_layer4_loss: 0.2667 - val_final_decode_fuse_loss: 0.0511 - val_Overall_fuse_conv_loss: 0.1232 - val_side_fuse_layer4_acc: 0.8772 - val_final_decode_fuse_my_iou_metric_non_zero: 0.5456 - val_Overall_fuse_conv_my_iou_metric: 0.7421\n",
      "\n",
      "Epoch 00034: final_decode_fuse_my_iou_metric_non_zero did not improve from 0.54627\n",
      "Epoch 35/100\n",
      "7180/7180 [==============================] - 33s 5ms/step - loss: 0.1267 - side_fuse_layer4_loss: 0.2626 - final_decode_fuse_loss: 0.0532 - Overall_fuse_conv_loss: 0.1082 - side_fuse_layer4_acc: 0.8868 - final_decode_fuse_my_iou_metric_non_zero: 0.5157 - Overall_fuse_conv_my_iou_metric: 0.7306 - val_loss: 0.1417 - val_side_fuse_layer4_loss: 0.2668 - val_final_decode_fuse_loss: 0.0511 - val_Overall_fuse_conv_loss: 0.1233 - val_side_fuse_layer4_acc: 0.8747 - val_final_decode_fuse_my_iou_metric_non_zero: 0.5456 - val_Overall_fuse_conv_my_iou_metric: 0.7393\n",
      "\n",
      "Epoch 00035: final_decode_fuse_my_iou_metric_non_zero did not improve from 0.54627\n",
      "Epoch 36/100\n",
      "7180/7180 [==============================] - 47s 7ms/step - loss: 0.1283 - side_fuse_layer4_loss: 0.2663 - final_decode_fuse_loss: 0.0533 - Overall_fuse_conv_loss: 0.1097 - side_fuse_layer4_acc: 0.8870 - final_decode_fuse_my_iou_metric_non_zero: 0.5153 - Overall_fuse_conv_my_iou_metric: 0.7295 - val_loss: 0.1420 - val_side_fuse_layer4_loss: 0.2668 - val_final_decode_fuse_loss: 0.0511 - val_Overall_fuse_conv_loss: 0.1235 - val_side_fuse_layer4_acc: 0.8772 - val_final_decode_fuse_my_iou_metric_non_zero: 0.5456 - val_Overall_fuse_conv_my_iou_metric: 0.7411\n",
      "\n",
      "Epoch 00036: final_decode_fuse_my_iou_metric_non_zero did not improve from 0.54627\n",
      "Epoch 37/100\n",
      "7180/7180 [==============================] - 37s 5ms/step - loss: 0.1312 - side_fuse_layer4_loss: 0.2640 - final_decode_fuse_loss: 0.0536 - Overall_fuse_conv_loss: 0.1126 - side_fuse_layer4_acc: 0.8865 - final_decode_fuse_my_iou_metric_non_zero: 0.5131 - Overall_fuse_conv_my_iou_metric: 0.7263 - val_loss: 0.1417 - val_side_fuse_layer4_loss: 0.2667 - val_final_decode_fuse_loss: 0.0511 - val_Overall_fuse_conv_loss: 0.1233 - val_side_fuse_layer4_acc: 0.8772 - val_final_decode_fuse_my_iou_metric_non_zero: 0.5454 - val_Overall_fuse_conv_my_iou_metric: 0.7411\n",
      "\n",
      "Epoch 00037: final_decode_fuse_my_iou_metric_non_zero did not improve from 0.54627\n",
      "Epoch 38/100\n",
      "7180/7180 [==============================] - 38s 5ms/step - loss: 0.1311 - side_fuse_layer4_loss: 0.2681 - final_decode_fuse_loss: 0.0534 - Overall_fuse_conv_loss: 0.1123 - side_fuse_layer4_acc: 0.8850 - final_decode_fuse_my_iou_metric_non_zero: 0.5118 - Overall_fuse_conv_my_iou_metric: 0.7271 - val_loss: 0.1415 - val_side_fuse_layer4_loss: 0.2667 - val_final_decode_fuse_loss: 0.0511 - val_Overall_fuse_conv_loss: 0.1231 - val_side_fuse_layer4_acc: 0.8747 - val_final_decode_fuse_my_iou_metric_non_zero: 0.5464 - val_Overall_fuse_conv_my_iou_metric: 0.7424\n",
      "\n",
      "Epoch 00038: final_decode_fuse_my_iou_metric_non_zero did not improve from 0.54627\n",
      "Epoch 39/100\n",
      "7180/7180 [==============================] - 23s 3ms/step - loss: 0.1269 - side_fuse_layer4_loss: 0.2661 - final_decode_fuse_loss: 0.0534 - Overall_fuse_conv_loss: 0.1082 - side_fuse_layer4_acc: 0.8850 - final_decode_fuse_my_iou_metric_non_zero: 0.5103 - Overall_fuse_conv_my_iou_metric: 0.7303 - val_loss: 0.1412 - val_side_fuse_layer4_loss: 0.2666 - val_final_decode_fuse_loss: 0.0510 - val_Overall_fuse_conv_loss: 0.1228 - val_side_fuse_layer4_acc: 0.8772 - val_final_decode_fuse_my_iou_metric_non_zero: 0.5459 - val_Overall_fuse_conv_my_iou_metric: 0.7398\n",
      "\n",
      "Epoch 00039: final_decode_fuse_my_iou_metric_non_zero did not improve from 0.54627\n",
      "Epoch 40/100\n",
      "7180/7180 [==============================] - 41s 6ms/step - loss: 0.1276 - side_fuse_layer4_loss: 0.2649 - final_decode_fuse_loss: 0.0533 - Overall_fuse_conv_loss: 0.1091 - side_fuse_layer4_acc: 0.8893 - final_decode_fuse_my_iou_metric_non_zero: 0.5125 - Overall_fuse_conv_my_iou_metric: 0.7289 - val_loss: 0.1418 - val_side_fuse_layer4_loss: 0.2666 - val_final_decode_fuse_loss: 0.0511 - val_Overall_fuse_conv_loss: 0.1233 - val_side_fuse_layer4_acc: 0.8797 - val_final_decode_fuse_my_iou_metric_non_zero: 0.5456 - val_Overall_fuse_conv_my_iou_metric: 0.7416\n",
      "\n",
      "Epoch 00040: final_decode_fuse_my_iou_metric_non_zero did not improve from 0.54627\n",
      "Epoch 41/100\n",
      "7180/7180 [==============================] - 46s 6ms/step - loss: 0.1298 - side_fuse_layer4_loss: 0.2667 - final_decode_fuse_loss: 0.0536 - Overall_fuse_conv_loss: 0.1111 - side_fuse_layer4_acc: 0.8872 - final_decode_fuse_my_iou_metric_non_zero: 0.5082 - Overall_fuse_conv_my_iou_metric: 0.7278 - val_loss: 0.1418 - val_side_fuse_layer4_loss: 0.2667 - val_final_decode_fuse_loss: 0.0511 - val_Overall_fuse_conv_loss: 0.1234 - val_side_fuse_layer4_acc: 0.8772 - val_final_decode_fuse_my_iou_metric_non_zero: 0.5456 - val_Overall_fuse_conv_my_iou_metric: 0.7414\n",
      "\n",
      "Epoch 00041: final_decode_fuse_my_iou_metric_non_zero did not improve from 0.54627\n",
      "Epoch 42/100\n",
      " 192/7180 [..............................] - ETA: 2:49 - loss: 0.1441 - side_fuse_layer4_loss: 0.3019 - final_decode_fuse_loss: 0.0544 - Overall_fuse_conv_loss: 0.1236 - side_fuse_layer4_acc: 0.8698 - final_decode_fuse_my_iou_metric_non_zero: 0.5349 - Overall_fuse_conv_my_iou_metric: 0.7172"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-37c09a035c80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                     callbacks=[model_checkpoint, reduce_lr],shuffle=True)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 64\n",
    "\n",
    "#lr_finder = LRFinder(min_lr=1e-5,max_lr=1e-2, steps_per_epoch=35, epochs=3)\n",
    "#model.compile(optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08))\n",
    "pred_out = model.fit(x_train, [y_train_hasmask,y_train,y_train],\n",
    "                    validation_data=(x_valid, [y_valid_hasmask,y_valid,y_valid]), \n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[model_checkpoint, reduce_lr],shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDRScheduler(Callback):\n",
    "    '''Cosine annealing learning rate scheduler with periodic restarts.\n",
    "    # Usage\n",
    "        ```python\n",
    "            schedule = SGDRScheduler(min_lr=1e-5,\n",
    "                                     max_lr=1e-2,\n",
    "                                     steps_per_epoch=np.ceil(epoch_size/batch_size),\n",
    "                                     lr_decay=0.9,\n",
    "                                     cycle_length=5,\n",
    "                                     mult_factor=1.5)\n",
    "            model.fit(X_train, Y_train, epochs=100, callbacks=[schedule])\n",
    "\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 min_lr,\n",
    "                 max_lr,\n",
    "                 steps_per_epoch,\n",
    "                 lr_decay=1,\n",
    "                 cycle_length=10,\n",
    "                 mult_factor=2):\n",
    "\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.lr_decay = lr_decay\n",
    "\n",
    "        self.batch_since_restart = 0\n",
    "        self.next_restart = cycle_length\n",
    "\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "\n",
    "        self.cycle_length = cycle_length\n",
    "        self.mult_factor = mult_factor\n",
    "\n",
    "        self.history = {}\n",
    "\n",
    "    def clr(self):\n",
    "        '''Calculate the learning rate.'''\n",
    "        fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n",
    "        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n",
    "        return lr\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
    "        logs = logs or {}\n",
    "        K.set_value(self.model.optimizer.lr, self.max_lr)\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        '''Record previous batch statistics and update the learning rate.'''\n",
    "        logs = logs or {}\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        self.batch_since_restart += 1\n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        '''Check for end of current cycle, apply restarts when necessary.'''\n",
    "        if epoch + 1 == self.next_restart:\n",
    "            self.batch_since_restart = 0\n",
    "            self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n",
    "            self.next_restart += self.cycle_length\n",
    "            self.max_lr *= self.lr_decay\n",
    "            self.best_weights = self.model.get_weights()\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        '''Set weights to the values from the end of the most recent cycle for best performance.'''\n",
    "        self.model.set_weights(self.best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"./keras.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule = SGDRScheduler(min_lr=1e-7,\n",
    "                                     max_lr=1e-4,\n",
    "                                     steps_per_epoch=3,\n",
    "                                     lr_decay=0.5,\n",
    "                                     cycle_length=5,\n",
    "                                     mult_factor=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected data to have shape (128, 128, 3) but got array with shape (128, 128, 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-3850b63b01fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                     callbacks=[model_checkpoint, schedule],shuffle=True)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1630\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1631\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m   1474\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1476\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1477\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1478\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    121\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected data to have shape (128, 128, 3) but got array with shape (128, 128, 4)"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "\n",
    "pred_out2 = model.fit(x_train, [y_train_hasmask,y_train,y_train],\n",
    "                    validation_data=(x_valid, [y_valid_hasmask,y_valid,y_valid]), \n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[model_checkpoint, schedule],shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25124 samples, validate on 399 samples\n",
      "Epoch 1/5\n",
      "25124/25124 [==============================] - 155s 6ms/step - loss: 0.0239 - side_fuse_layer4_loss: 0.0036 - final_decode_fuse_loss: 0.0110 - Overall_fuse_conv_loss: 0.0226 - side_fuse_layer4_acc: 0.9998 - final_decode_fuse_my_iou_metric: 0.9344 - Overall_fuse_conv_my_iou_metric: 0.8840 - val_loss: 0.1175 - val_side_fuse_layer4_loss: 0.3895 - val_final_decode_fuse_loss: 0.0843 - val_Overall_fuse_conv_loss: 0.0896 - val_side_fuse_layer4_acc: 0.9298 - val_final_decode_fuse_my_iou_metric: 0.8095 - val_Overall_fuse_conv_my_iou_metric: 0.7957\n",
      "\n",
      "Epoch 00001: val_final_decode_fuse_my_iou_metric improved from -inf to 0.80952, saving model to ./keras.model\n",
      "Epoch 2/5\n",
      "25124/25124 [==============================] - 148s 6ms/step - loss: 0.0235 - side_fuse_layer4_loss: 0.0043 - final_decode_fuse_loss: 0.0108 - Overall_fuse_conv_loss: 0.0222 - side_fuse_layer4_acc: 0.9997 - final_decode_fuse_my_iou_metric: 0.9348 - Overall_fuse_conv_my_iou_metric: 0.8857 - val_loss: 0.1189 - val_side_fuse_layer4_loss: 0.4128 - val_final_decode_fuse_loss: 0.0841 - val_Overall_fuse_conv_loss: 0.0899 - val_side_fuse_layer4_acc: 0.9248 - val_final_decode_fuse_my_iou_metric: 0.8113 - val_Overall_fuse_conv_my_iou_metric: 0.7967\n",
      "\n",
      "Epoch 00002: val_final_decode_fuse_my_iou_metric improved from 0.80952 to 0.81128, saving model to ./keras.model\n",
      "Epoch 3/5\n",
      "25124/25124 [==============================] - 163s 6ms/step - loss: 0.0231 - side_fuse_layer4_loss: 0.0035 - final_decode_fuse_loss: 0.0106 - Overall_fuse_conv_loss: 0.0218 - side_fuse_layer4_acc: 0.9999 - final_decode_fuse_my_iou_metric: 0.9360 - Overall_fuse_conv_my_iou_metric: 0.8865 - val_loss: 0.1181 - val_side_fuse_layer4_loss: 0.4189 - val_final_decode_fuse_loss: 0.0832 - val_Overall_fuse_conv_loss: 0.0889 - val_side_fuse_layer4_acc: 0.9298 - val_final_decode_fuse_my_iou_metric: 0.8080 - val_Overall_fuse_conv_my_iou_metric: 0.7977\n",
      "\n",
      "Epoch 00003: val_final_decode_fuse_my_iou_metric did not improve from 0.81128\n",
      "Epoch 4/5\n",
      "25124/25124 [==============================] - 155s 6ms/step - loss: 0.0230 - side_fuse_layer4_loss: 0.0035 - final_decode_fuse_loss: 0.0106 - Overall_fuse_conv_loss: 0.0218 - side_fuse_layer4_acc: 0.9998 - final_decode_fuse_my_iou_metric: 0.9365 - Overall_fuse_conv_my_iou_metric: 0.8880 - val_loss: 0.1178 - val_side_fuse_layer4_loss: 0.3980 - val_final_decode_fuse_loss: 0.0838 - val_Overall_fuse_conv_loss: 0.0895 - val_side_fuse_layer4_acc: 0.9348 - val_final_decode_fuse_my_iou_metric: 0.8105 - val_Overall_fuse_conv_my_iou_metric: 0.8038\n",
      "\n",
      "Epoch 00004: val_final_decode_fuse_my_iou_metric did not improve from 0.81128\n",
      "Epoch 5/5\n",
      "25124/25124 [==============================] - 156s 6ms/step - loss: 0.0227 - side_fuse_layer4_loss: 0.0030 - final_decode_fuse_loss: 0.0105 - Overall_fuse_conv_loss: 0.0215 - side_fuse_layer4_acc: 0.9999 - final_decode_fuse_my_iou_metric: 0.9366 - Overall_fuse_conv_my_iou_metric: 0.8889 - val_loss: 0.1137 - val_side_fuse_layer4_loss: 0.3817 - val_final_decode_fuse_loss: 0.0805 - val_Overall_fuse_conv_loss: 0.0865 - val_side_fuse_layer4_acc: 0.9273 - val_final_decode_fuse_my_iou_metric: 0.8095 - val_Overall_fuse_conv_my_iou_metric: 0.8005\n",
      "\n",
      "Epoch 00005: val_final_decode_fuse_my_iou_metric did not improve from 0.81128\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "early_stopping = EarlyStopping(patience=10, verbose=1)\n",
    "model_checkpoint = ModelCheckpoint(\"./keras.model\", monitor='val_final_decode_fuse_my_iou_metric',mode = 'max',  save_best_only=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00000001, verbose=1)\n",
    "pred_out2 = model.fit(x_train, [y_train_hasmask,y_train,y_train],\n",
    "                    validation_data=(x_valid, [y_valid_hasmask,y_valid,y_valid]), \n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[model_checkpoint, schedule],shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"./keras.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preds_test1,preds_test2,preds_test3 = model.predict(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test1_new = ((preds_test1>0.5)*1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_flag = []\n",
    "ids_valid_flag = []\n",
    "for i in range(len(y_valid)):\n",
    "    y_valid_flag.append((y_valid[i,:,:,0].sum()>0)*1)\n",
    "    if y_valid_flag[i]>0:\n",
    "        ids_valid_flag.append(ids_valid[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "cf = sklearn.metrics.confusion_matrix(y_valid_flag, preds_test1_new, labels=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[146,  17],\n",
       "        [ 10, 226]]), (236, 128, 128, 3), 236)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf,x_valid_maskonly.shape, len(ids_valid_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get non_zero images for prediction by pred_test2\n",
    "x_valid_maskonly =[]\n",
    "for i in range(len(y_valid_flag)):\n",
    "    if y_valid_flag[i]>0:\n",
    "        x_valid_maskonly.append(x_valid[i,:,:,:])\n",
    "\n",
    "x_valid_maskonly = np.array(x_valid_maskonly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_result(model,x_test,img_size_target): # predict both orginal and reflect x\n",
    "    \n",
    "    preds_test1 = model.predict(x_test)\n",
    "    \n",
    "    '''\n",
    "    #test time augmentation \n",
    "    \n",
    "    AUG_NR = 6\n",
    "    aug_imgs_tta = []\n",
    "    for i in range(len(x_test)-1):\n",
    "        for _ in range(AUG_NR):\n",
    "            img_x = seq_det.augment_image(x_test[i])\n",
    "            aug_imgs_tta.append(img_x)\n",
    "\n",
    "    aug_imgs_tta = np.array(aug_imgs_tta)\n",
    "    # x_train = np.append(x_train,aug_imgs_x,axis=0)\n",
    "    '''\n",
    "    x_test_flip =  np.array([np.fliplr(x) for x in x_test])\n",
    "    preds_test2_flip = model.predict(x_test_flip)\n",
    "    preds_test2 = np.array([ np.fliplr(x) for x in preds_test2_flip[2]] )\n",
    "     \n",
    "    preds_avg = (preds_test1[2] +preds_test2[2])/2\n",
    "    \n",
    "   # del x_test_flip , preds_test1, preds_test2_flip\n",
    "    print(len(preds_avg))\n",
    "    \n",
    "    return preds_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((399, 128, 128, 3), (236, 128, 128, 3))"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid.shape , x_valid_maskonly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preds_valid = predict_result(model,x_valid,img_size_target)#.reshape(-1, img_size_target, img_size_target)\n",
    "preds_valid = model.predict(x_valid)#.reshape(-1, img_size_target, img_size_target)\n",
    "\n",
    "classify=[]\n",
    "classify = (preds_valid[0]>0.5)*1\n",
    "\n",
    "preds_valid = preds_valid[1]\n",
    "preds_valid_new = []\n",
    "for i in range(len(x_valid)):\n",
    "    preds_valid_new.append(classify[i] * preds_valid[i,:,:,0])\n",
    "\n",
    "preds_valid = np.array(preds_valid_new).reshape(-1, img_size_target, img_size_target)\n",
    "#preds_valid_hr = np.array([np.fliplr(x) for x in x_test])\n",
    "#preds_valid = predictions\n",
    "preds_valid = np.array([downsample(x) for x in preds_valid])\n",
    "y_valid_ori = np.array([train_df.loc[idx].masks for idx in ids_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((399, 101, 101), (399, 101, 101))"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_valid.shape,y_valid_ori.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intersection over union\n",
    "def iou_metric(y_true_in, y_pred_in, print_table=False):\n",
    "    labels = y_true_in\n",
    "    y_pred = y_pred_in\n",
    "    \n",
    "    true_objects = 2\n",
    "    pred_objects = 2\n",
    "\n",
    "    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n",
    "\n",
    "    # Compute areas (needed for finding the union between all objects)\n",
    "    area_true = np.histogram(labels, bins = true_objects)[0]\n",
    "    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n",
    "    area_true = np.expand_dims(area_true, -1)\n",
    "    area_pred = np.expand_dims(area_pred, 0)\n",
    "\n",
    "    # Compute union\n",
    "    union = area_true + area_pred - intersection\n",
    "\n",
    "    # Exclude background from the analysis\n",
    "    intersection = intersection[1:,1:]\n",
    "    union = union[1:,1:]\n",
    "    union[union == 0] = 1e-9\n",
    "\n",
    "    # Compute the intersection over union\n",
    "    iou = intersection / union\n",
    "\n",
    "    # Precision helper function\n",
    "    def precision_at(threshold, iou):\n",
    "        matches = iou > threshold\n",
    "        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n",
    "        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n",
    "        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n",
    "        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n",
    "        return tp, fp, fn\n",
    "\n",
    "    # Loop over IoU thresholds\n",
    "    prec = []\n",
    "    if print_table:\n",
    "        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        tp, fp, fn = precision_at(t, iou)\n",
    "        if (tp + fp + fn) > 0:\n",
    "            p = tp / (tp + fp + fn)\n",
    "        else:\n",
    "            p = 0\n",
    "        if print_table:\n",
    "            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n",
    "        prec.append(p)\n",
    "    \n",
    "    if print_table:\n",
    "        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n",
    "    return np.mean(prec)\n",
    "\n",
    "def iou_metric_batch(y_true_in, y_pred_in):\n",
    "    batch_size = y_true_in.shape[0]\n",
    "    metric = []\n",
    "    for batch in range(batch_size):\n",
    "        value = get_iou_vector(y_true_in[batch], y_pred_in[batch])\n",
    "        #print(value)\n",
    "        metric.append(value)\n",
    "    return np.mean(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0, 1, 50)\n",
    "ious = np.array([iou_metric_batch(y_valid_ori, np.int32(preds_valid > threshold)) for threshold in thresholds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_best_index = np.argmax(ious[9:-10]) + 9\n",
    "iou_best = ious[threshold_best_index]\n",
    "threshold_best = thresholds[threshold_best_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f37a6f98518>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(thresholds, ious)\n",
    "plt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"IoU\")\n",
    "plt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RLenc(img, order='F', format=True):\n",
    "    \"\"\"\n",
    "    img is binary mask image, shape (r,c)\n",
    "    order is down-then-right, i.e. Fortran\n",
    "    format determines if the order needs to be preformatted (according to submission rules) or not\n",
    "\n",
    "    returns run length as an array or string (if format is True)\n",
    "    \"\"\"\n",
    "    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n",
    "    runs = []  ## list of run lengths\n",
    "    r = 0  ## the current run length\n",
    "    pos = 1  ## count starts from 1 per WK\n",
    "    for c in bytes:\n",
    "        if (c == 0):\n",
    "            if r != 0:\n",
    "                runs.append((pos, r))\n",
    "                pos += r\n",
    "                r = 0\n",
    "            pos += 1\n",
    "        else:\n",
    "            r += 1\n",
    "\n",
    "    # if last run is unsaved (i.e. data ends with 1)\n",
    "    if r != 0:\n",
    "        runs.append((pos, r))\n",
    "        pos += r\n",
    "        r = 0\n",
    "\n",
    "    if format:\n",
    "        z = ''\n",
    "\n",
    "        for rr in runs:\n",
    "            z += '{} {} '.format(rr[0], rr[1])\n",
    "        return z[:-1]\n",
    "    else:\n",
    "        return runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sourav/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "test_df[\"test\"] = [np.array(load_img(\"test/{}.png\".format(idx),grayscale = True)) / 255 for idx in test_df.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=np.array(test_df.test.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x_test = np.zeros((len(x_test), img_size_target, img_size_target, 3))\n",
    "for i in range(x_test.shape[0]):\n",
    "    new_x_test[i,:,:,0] =  x_test[i,: ,:,0]\n",
    "    h,w,n = x_test[i,: ,:,:].shape\n",
    "    for row, const in enumerate(np.linspace(0, 1, h)):\n",
    "        #print(const)\n",
    "        new_x_test[i,row, : ,1] = const\n",
    "    new_x_test[i,:,:,2] = new_x_test[i,:,:,0] * new_x_test[i,:,:,1]\n",
    "    #img = add_depth_channels(x_train[i,: ,:,:])\n",
    "\n",
    "x_test = new_x_test\n",
    "del new_x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000, 128, 128, 3)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = model.predict(x_test)#.reshape(-1, img_size_target, img_size_target)\n",
    "#preds_test = predict_result(model,x_test,img_size_target)\n",
    "#classify_test=[]\n",
    "#classify_test = (preds_test[0]>0.5)*1\n",
    "#len(classify_test)\n",
    "\n",
    "preds_test = preds_test[2]\n",
    "preds_test_new = []\n",
    "for i in range(len(x_test)):    \n",
    "    preds_test_new.append(classify_test[i] * preds_test[i,:,:,0])\n",
    "\n",
    "preds_test = np.array(preds_test_new).reshape(-1, img_size_target, img_size_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "del preds_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = {idx: RLenc(np.round(downsample(preds_test[i]) > threshold_best)) for i, idx in enumerate(test_df.index.values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame.from_dict(pred_dict,orient='index')\n",
    "sub.index.names = ['id']\n",
    "sub.columns = ['rle_mask']\n",
    "sub.to_csv('submission8_b963c_bestthre.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict=None\n",
    "pred_test=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
